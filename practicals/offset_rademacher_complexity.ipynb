{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offset_rademacher_complexities.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2cTRs5JmF0P2",
        "yRWyjpBQfqS9"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0tSQOaa_Zzb"
      },
      "source": [
        "# Offset Rademacher Complexity\n",
        "\n",
        "<font color='green'> In this practical session, we introduce the idea of localized Rademacher complexity measures through a simple example of offset Rademacher complexity. Our main objectives are the following:\n",
        "</font>\n",
        "- <font color='green'>revisiting the symmetrization and contraction techniques leading to Rademacher complexity upper bounds on the estimation error;</font>\n",
        "- <font color='green'>understanding why Rademacher complexity upper bounds may fail to capture the correct rate of convergence;</font>\n",
        "- <font color='green'>gaining an intuitive understanding of ways to capture the correct rate.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk1ltgcSmIdO"
      },
      "source": [
        "In the lectures, we have seen how the notion of Rademacher complexity can be used to provide upper bounds on the estimation error of learning algorithms aimed at minimizing the empirical risk. For simplicity, in this practical session, we will investigate a slightly easier problem of bounding the *expected* estimation error. The aim of this practical session is to demonstrate some of the limitations of Rademacher complexity upper bound\n",
        "as well as potential ways to resolve them.\n",
        "\n",
        "We now recall the general setup of a statistical learning problem. Let $Z_{1}^{n} = (Z_{1}, \\dots, Z_{n})$ denote an i.i.d. sample of $n$ input-output pairs $Z_{i} = (X_{i}, Y_{i}) \\in \\mathcal{X} \\times \\mathcal{Y}$ sampled from some unknown distribution $P$. Let $\\mathcal{A}$ denote some function class of interest mapping $\\mathcal{X}$ to $\\mathcal{Y}$. Denote any empirical risk minimizer among the functions in $\\mathcal{A}$ by\n",
        "$$\n",
        "  A = A^{(ERM)} \\in \\mathrm{argmin}_{a \\in \\mathcal{A}} R(a),\\quad\\text{where}\\quad\n",
        "  R(a) = \\frac{1}{n} \\sum_{i=1}^{n}\\ell(a, Z_{i})\n",
        "$$\n",
        "for some loss function $\\ell : \\mathcal{A} \\times \\mathcal{Y} \\to [0, \\infty)$.\n",
        "\n",
        "Let $\\Omega_{1}, \\dots, \\Omega_{n}$ denote $n$ i.i.d. Rademacher random variables (i.e., independent symmetric $\\{-1, +1\\}$ valued random variables).\n",
        "The *empirical Rademacher complexity* of the function class $\\mathcal{A}$ conditionally on the observed covariates\n",
        "$X_{1} = x_{1}, \\dots, X_{n} = x_{n}$ is defined by\n",
        "$$\n",
        "    \\mathrm{Rad}(\\mathcal{A} \\circ \\{x_{1}, \\dots, x_{n}\\})\n",
        "    = \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "      \\sup_{a \\in \\mathcal{A}} \\left\\{\n",
        "        \\frac{1}{n}\\sum_{i=1}^{n}\\Omega_{i}a(x_{i})\n",
        "      \\right\\}\n",
        "    \\right].\n",
        "$$\n",
        "\n",
        "If the loss function $\\ell$ is $\\gamma$-Lipschitz in the sense that\n",
        "$$\n",
        "  \\forall a, a' \\in \\mathcal{A} \\text{ and } \\forall z \\in \\mathcal{Z} = \\mathcal{X} \\times \\mathcal{Y}\\text{ it holds that } \n",
        "  \\left|\\ell(a, z) - \\ell(a', z)\\right| \\leq \\gamma \\left|a(x) - a'(x)\\right|,\n",
        "$$\n",
        "then the expected estimation error of any empirical risk minimizer $A$ can be upper bounded by\n",
        "\\begin{align}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - \\inf_{a \\in \\mathcal{A}} r(a)\n",
        "  \\right]\n",
        "  &\\leq 2 \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "      \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "          \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "            \\frac{1}{n}\\sum_{i=1}^{n}\\Omega_{i}\\ell(a, Z_{i}) \n",
        "          \\right\\}\n",
        "          \\bigg\\vert Z_{1}, \\dots, Z_{n}\n",
        "      \\right]\n",
        "  \\right]\n",
        "  \\\\\n",
        "  &\\leq \n",
        "  2\\gamma \\mathbf{E}_{X_{1}^{n}}\\left[\n",
        "      \\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\}\n",
        "  \\right],\n",
        "\\end{align}\n",
        "where the final step follows via Ledoux-Talagrand contraction inequality.\n",
        "Before revisiting the proof leading to the above expected estimation error upper bound, we turn to the next section where we are asked to compute the Rademacher complexity in the special case when $\\mathcal{A}$ is a class of linear functions bounded in $\\ell_{2}$ norm.\n",
        "\n",
        "---\n",
        "\n",
        "**Setup**\n",
        "\n",
        "\n",
        ">In the rest of this practical session, we fix the following setup:\n",
        ">\\begin{align}\n",
        "    \\mathcal{X} &= \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq 1\\}, \\\\\n",
        "    \\mathcal{Y} &= [-2, 2], \\\\\n",
        "    \\mathcal{A} &= \\{a_{w}(\\cdot) = \\langle w, \\cdot \\rangle : w \\in \\mathbb{R}^{d}, \\|w\\|_{2} \\leq 1 \\}, \\\\\n",
        "    \\ell(a_{w}, z) &= (\\langle w, x \\rangle - y)^{2}.\n",
        "\\end{align}\n",
        "Notice that within the above setp, the quadratic loss is $6$-Lipschitz (i.e. $\\gamma = 6$), and in particular, for any data generating distribution $P$ supported on $\\mathcal{X} \\times \\mathcal{Y}$ we have\n",
        "$$\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - \\inf_{a \\in \\mathcal{A}} r(a)\n",
        "  \\right]\n",
        "  \\leq \n",
        "  12 \\mathbf{E}_{X_{1}^{n}}\\left[\n",
        "      \\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\}\n",
        "  \\right].\n",
        "$$\n",
        "><font color='green'>**All of the results to follow will be stated in force of the setup outlined above. We have chosen the above setting to provide a simple example of an easy learning problem for which Rademacher complexity measure fails to capture the correct estimation error convergence rate. Analyzing this example will allow us to better understand why localization of Rademacher complexities is necessary to obtain tight upper bounds on the estimation error.**</font>\n",
        ">\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwg2HhxSKDz"
      },
      "source": [
        "## Rademacher Complexity of Bounded Linear Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhG1FCTi6BP"
      },
      "source": [
        "In Exercise 1, we will compute an upper bound on the Rademacher complexity of linear predictors. To empirically verify whether the proved upper bound is tight, we will first implement a function that allows us to obtain Monte Carlo estimates of $\\mathbf{E}_{X_{1}^{n}}[\\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\})]$.\n",
        "For our simulations, we fix the marginal distribution of the covariates to be uniform over the unit sphere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtJjKxM7sxsk"
      },
      "source": [
        "import numpy as np # For manimpulating arrays.\n",
        "from matplotlib import pyplot as plt # For potting.\n",
        "from sklearn.linear_model import LinearRegression # For computing A."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYp7E9Mgrw-o"
      },
      "source": [
        "The below function implements the sampling of random covariate vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdY4dc-YqYKA"
      },
      "source": [
        "def sample_from_P_X(n, d):\n",
        "  \"\"\" Returns an n \\times d numpy whose i-th row is equal to a d-dimensional\n",
        "  vector, sampled uniformly from the unit sphere. \"\"\"\n",
        "  X = np.random.normal(loc=0, scale=1, size=(n,d))\n",
        "  l2_norms = np.sqrt(np.sum(X**2, axis=1)).reshape(-1,1)\n",
        "  return X / l2_norms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTWfiZQ6sCLc"
      },
      "source": [
        "The below function implements sampling Rademacher random variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF9akQZQsZDw"
      },
      "source": [
        "def sample_Omegas(n):\n",
        "  \"\"\" Returns an n dimensional numpy array whose i-th element is equal to an\n",
        "  independent sample of a Rademacher random variable. \"\"\"\n",
        "  return np.random.binomial(n=1, p=0.5, size=(n,))*2.0 - 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFXOsu12vgT"
      },
      "source": [
        "We can now implement a function that returns Monte Carlo samples of $\\mathbf{E}_{X_{1}^{n}}[\\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\})]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT9i-tFe3Tz9"
      },
      "source": [
        "def sample_Rad(n, d):\n",
        "  \"\"\" :n: Number of data points.\n",
        "      :d: Dimension of the covariates.\n",
        "      :returns: A tuple (w_opt, rad), where w_opt is a d-dimensional vector\n",
        "        maximizing \\frac{1}{n}\\sum_{i=1}^{n} Omega_i * <w, X_i> and rad is \n",
        "        a 1-sample Monte Carlo estimate of E[Rad(A \\circ {X_1, \\dots, X_n})].\n",
        "  \"\"\"\n",
        "  X = sample_from_P_X(n, d)\n",
        "  Omega = sample_Omegas(n)\n",
        "  # We now need to compute\n",
        "  #  \\sup_{w : ||w||_{2} \\leq 1} \\frac{1}{n} \\sum_{i=1}^{n} Omega_i * <w, X_i>.\n",
        "  sum_Omega_i_X_i = np.average(X * Omega.reshape(-1, 1), axis=0)\n",
        "  sum_Omega_i_X_i_l2_norm = np.sqrt(np.sum(sum_Omega_i_X_i**2))\n",
        "  # The optimum is attained by a unit vector perfectly aligned with\n",
        "  # \\sum_{i=1}^{n} Omega_i * X_i.\n",
        "  w_opt = sum_Omega_i_X_i / sum_Omega_i_X_i_l2_norm\n",
        "  return w_opt, sum_Omega_i_X_i_l2_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kVGFTFJC7sl"
      },
      "source": [
        "Below, we show how to use the `sample_Rad` function to empirically evaluate how\n",
        "$\\mathbf{E}_{X_{1}^{n}}[\\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\})]$ scales with respect to the sample size $n$. We fix the dimension $d=10$ in the below simulations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ_05W7s4TOg"
      },
      "source": [
        "ns = np.arange(start=100, stop=201, step=10)\n",
        "d = 10\n",
        "n_monte_carlo_samples = 2500\n",
        "rad_estimates = np.zeros(len(ns))\n",
        "\n",
        "np.random.seed(0)\n",
        "for idx, n in enumerate(ns):\n",
        "  for _ in range(n_monte_carlo_samples):\n",
        "    rad_estimates[idx] += sample_Rad(n, d)[1] / n_monte_carlo_samples\n",
        "\n",
        "plt.plot(np.log(ns), np.log(rad_estimates))\n",
        "plt.xlabel(r'$\\log n$')\n",
        "plt.ylabel(r'$\\log \\mathbf{E} \\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\})$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjhYxQOnE6P4"
      },
      "source": [
        "The slope in the above generated plot is approximately equal to $-0.5$. This suggests that\n",
        "$\\mathbf{E}_{X_{1}^{n}}[\\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\}]$\n",
        "scales as $O(1/\\sqrt{n})$ (within our setup described at the beginning of this notebook). The next exercise asks you to prove this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXMN-QuY_nhu"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QFAM3-2eBGl"
      },
      "source": [
        "Consider the problem setup described in the introduction. Prove that for any dimension $d$ of covariate vectors we have\n",
        "$$\n",
        "  \\mathbf{E}_{X_{1}^{n}}[\n",
        "    \\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\})\n",
        "  ] \\leq \\frac{1}{\\sqrt{n}}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cTRs5JmF0P2"
      },
      "source": [
        "#### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSJkQ41IF10W"
      },
      "source": [
        "You may it helpful to recall [Cauchy-Schwarz](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality) and [Jensen's](https://en.wikipedia.org/wiki/Jensen%27s_inequality) inequalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRWyjpBQfqS9"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36R_0xgwFpiO"
      },
      "source": [
        "For any $x_{1}, \\dots, x_{n} \\in \\mathcal{X}$ we have\n",
        "\\begin{align*}\n",
        "  \\mathrm{Rad}(\\mathcal{A} \\circ \\{ x_{1}, \\dots, x_{n} \\})\n",
        "  &= \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "    \\sup_{w : \\|w\\| \\leq 1} \\left\\{\n",
        "      \\frac{1}{n}\\sum_{i=1}^{n}\\Omega_{i} \\langle w, x_{i} \\rangle\n",
        "    \\right\\}\n",
        "  \\right]\n",
        "  \\\\\n",
        "  &= \\frac{1}{n} \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "    \\sup_{w : \\|w\\| \\leq 1} \\left\\{\n",
        "      \\left\\langle w, \\sum_{i=1}^{n} \\Omega_{i} x_{i} \\right\\rangle\n",
        "    \\right\\}\n",
        "  \\right]\n",
        "  \\\\\n",
        "  &\\leq \\frac{1}{n} \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "    \\sup_{w : \\|w\\| \\leq 1} \\left\\{\n",
        "      \\left\\|w\\right\\|_{2}\n",
        "      \\left\\| \\sum_{i=1}^{n} \\Omega_{i} x_{i} \\right\\|_{2}\n",
        "    \\right\\}\n",
        "  \\right]\n",
        "  &\\text{by Cauchy-Schwarz inequality}\n",
        "  \\\\\n",
        "  &\\leq \\frac{1}{n} \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "      \\left\\| \\sum_{i=1}^{n} \\Omega_{i} x_{i} \\right\\|_{2}\n",
        "  \\right]\n",
        "  \\\\\n",
        "  &\\leq \\frac{1}{n}\n",
        "      \\sqrt{\n",
        "        \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "        \\left\\| \\sum_{i=1}^{n} \\Omega_{i} x_{i} \\right\\|_{2}^{2}\n",
        "        \\right]\n",
        "       }\n",
        "  &\\text{by Jensen's inequality}\n",
        "  \\\\\n",
        "  &= \\frac{1}{n}\n",
        "      \\sqrt{\n",
        "        \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "          \\sum_{i,j} \\Omega_{i}\\Omega_{j} \\langle x_{i}, x_{j}\\rangle\n",
        "        \\right]\n",
        "       }\n",
        "  \\\\\n",
        "  &= \\frac{1}{n}\n",
        "      \\sqrt{\n",
        "        \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "          \\sum_{i} \\Omega_{i}^{2} \\langle x_{i}, x_{i}\\rangle\n",
        "        \\right]\n",
        "       }\n",
        "  \\\\\n",
        "  &= \\frac{1}{n}\n",
        "      \\sqrt{\n",
        "          \\sum_{i} \\|x_{i}\\|_{2}^{2}\n",
        "       }\n",
        "  \\\\\n",
        "  &\\leq \\frac{1}{n}\n",
        "      \\sqrt{\n",
        "          \\sum_{i} 1\n",
        "       }\n",
        "  \\\\\n",
        "  &= \\frac{1}{\\sqrt{n}}.\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-uXtZm4e7HU"
      },
      "source": [
        "## Rademacher Complexity Upper Bounds on the Estimation Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVE8ne93a9nn"
      },
      "source": [
        "In the introduction, we have discussed that Rademacher complexity leads to the following upper bound on the expected estimation error (within the setup described at the beginning of this notebook):\n",
        "$$\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - \\inf_{a \\in \\mathcal{A}} r(a)\n",
        "  \\right]\n",
        "  \\leq \n",
        "  12 \\mathbf{E}_{X_{1}^{n}}\\left[\n",
        "      \\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\})\n",
        "  \\right].\n",
        "$$\n",
        "<font color='green'>**Let us now devise an experiment to verify how tight the above upper bound is; specifically, we want to find out if the left hand side of the above inequality indeed decreases as $1/\\sqrt{n}$.**</font>\n",
        "\n",
        "We will use the already implemented function `sample_from_P_X` to sample the covariate vectors. Conditionally on the covariate vectors, we will generate the\n",
        "labels as follows:\n",
        "$$\n",
        "  Y \\vert X = x \\sim \\langle w^{\\star}, x \\rangle + \\underbrace{\\mathrm{Uniform}[-1, 1]}_{\\text{noise}},\n",
        "  \\text{ where } w^{\\star} = \\frac{1}{2}(1/\\sqrt{d}, \\dots, 1/\\sqrt{d})^{\\mathsf{T}}.\n",
        "$$\n",
        "Notice that the above choice of $P$ ensures that $Y \\in [-2, 2]$ with probability $1$.\n",
        "The following function implements the conditional distribution of the response variable $Y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip0MHLJXdIGF"
      },
      "source": [
        "def sample_Y_given_X(X, w_star):\n",
        "  \"\"\" :X: An n \\times d numpy array with the i-th row equal to the i-th\n",
        "        covariate vector X_i.\n",
        "      :return: An n \\times 1 numpy array, with the i-th element equal to\n",
        "        a sample <w_star, X_i> + Uniform[-1, 1].\n",
        "  \"\"\"\n",
        "  n = X.shape[0]\n",
        "  return X @ w_star + np.random.uniform(low=-1.0, high=1.0, size=(n, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqH1w66FeCma"
      },
      "source": [
        "In the next cell we implement a function for generating datasets to be used in our simulations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7NjX5PdeOuN"
      },
      "source": [
        "def get_w_star(d):\n",
        "  \"\"\" Returns the parameter w* which represents the optimal linear function. \"\"\"\n",
        "  return np.ones((d, 1)) * 0.5 / np.sqrt(d)\n",
        "\n",
        "def generate_dataset(n, d):\n",
        "  \"\"\" Generates a dataset to be used in linear regression simulations, sampled\n",
        "  from the distribution described in the text above.\n",
        "    :n: Number of data points.\n",
        "    :d: Dimension of the covariates.\n",
        "    :returns: The dataset X, y, where X is a numpy array of shape n \\times d\n",
        "      with the i-th row equal to X_i and y is a numpy array of shape n \\times 1\n",
        "      with the i-th element equal to Y_i.\n",
        "  \"\"\"\n",
        "  X = sample_from_P_X(n, d)\n",
        "  w_star = get_w_star(d)\n",
        "  y = sample_Y_given_X(X, w_star)\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ja4otNIgXeQ"
      },
      "source": [
        "We now turn to the next exercise, which asks us to verify how well Rademacher complexities capture the scale of true expected estimation error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XZ6_8uZ_phb"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16puTWQWg1to"
      },
      "source": [
        "Recall that $A$ denotes any empirical risk minimizer in the set $\\mathcal{A}$ (see the beginning of this notebook for the exact setup).\n",
        "- Implement a function `sample_estimation_error` for computing Monte Carlo estimates of the expected estimation error\n",
        "$$\n",
        "  \\mathbf{E}_{Z_{1}^{n}}[r(A) - \\inf_{a \\in \\mathcal{A}} r(a)]\n",
        "  = \\mathbf{E}_{Z_{1}^{n}}[r(A) - r(\\langle w^{\\star}, \\cdot \\rangle)]\n",
        "$$\n",
        "- For the range of sample sizes given by the variable `ns` (defined above) and\n",
        "$d = 10$, generate a log-log plot of the estimated expected estimation error.\n",
        "How fast does the expected estimation error decay with respect to the sample size $n$?\n",
        "\n",
        "*Remark.* Due to the choice of our data generating distribution, for large enough sample sizes $n$ empirical risk minimization over $\\mathcal{A}$ will coincide (with high probability) with empirical risk minimization over the unconstrained space $\\{\\langle w, \\cdot \\rangle : w \\in \\mathbb{R}^{d}\\}$.\n",
        "Hence, in this exercise, you may compute any empirical risk minimizer over the unconstrained space (i.e., the ordinary least squares (OLS) estimator). You may compute the OLS estimator either directly or via the following package:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cba_toyka1-"
      },
      "source": [
        "def sample_estimation_error(n, d):\n",
        "  \"\"\" :n: Number of data points.\n",
        "      :d: Dimension of the covariates.\n",
        "      :returns: A tuple (A, estimation_error), where A is a d-dimensional vector\n",
        "        maximizing the empirical risk over \\mathcal{A}, and estimation_error is\n",
        "        equal to r(A) - r(<w*, .>), i.e., a 1-sample Monte Carlo estimate of\n",
        "        E[r(A) - r(<w*,.>)].\n",
        "  \"\"\"\n",
        "  ##############################################################################\n",
        "  # Exercise 2. Fill in the below code.\n",
        "  \n",
        "  ##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNm60yMJfod6"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZXSbsiErRZv"
      },
      "source": [
        "The function `sample_estimation_error` may be implemented as follows:\n",
        "```\n",
        "def sample_estimation_error(n, d):\n",
        "  X, y = generate_dataset(n, d)\n",
        "  reg = LinearRegression(fit_intercept=False)\n",
        "  reg.fit(X, y)\n",
        "  A = reg.coef_.reshape(-1, 1)\n",
        "  # The estimation error is equal to\n",
        "  # (A - w*)^t * E[XX^t] (A - w*) = ||A-w*||_2^2 * 1/d.\n",
        "  w_star = get_w_star(d)\n",
        "  estimation_error = np.average((A - w_star)**2)\n",
        "  return A, estimation_error\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8shk6c80raF8"
      },
      "source": [
        "We can now generate Monte Carlo estimates of the expected estimation error and compare them with Rademacher complexity upper bounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x7zfZ6-rkqA"
      },
      "source": [
        "d = 10\n",
        "n_monte_carlo_samples = 2500\n",
        "estimation_error_estimates = np.zeros(len(ns))\n",
        "\n",
        "np.random.seed(0)\n",
        "for idx, n in enumerate(ns):\n",
        "  for _ in range(n_monte_carlo_samples):\n",
        "    estimation_error_estimates[idx] += \\\n",
        "      sample_estimation_error(n, d)[1] / n_monte_carlo_samples\n",
        "\n",
        "plt.plot(np.log(ns), np.log(estimation_error_estimates))\n",
        "plt.plot(np.log(ns), np.log(rad_estimates))\n",
        "plt.xlabel(r'$\\log n$')\n",
        "plt.legend(['expected estimation error', 'rademacher complexity'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533CgNtpwUWF"
      },
      "source": [
        "<font color='green'>**From the above plot, we see that the expected estimation error appears to decay as $O(1/n)$, while the Rademacher complexity decays only as $O(1/\\sqrt{n})$.\n",
        "We conclude that Rademacher complexity fails to capture the true behaviour in the above example.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpu1FL_A_sfG"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2vyC_HylvCJ"
      },
      "source": [
        "In Exercise 2, we have found a mismatch between the true estimation error and the upper bounds offered by Rademacher complexities. To gain some intuition as to what went wrong, perform the following simulation:\n",
        "  1. For visualization purposes, fix $d = 2$.\n",
        "  2. Plot a disc of radius 1, representing the function class $\\mathcal{A}$.\n",
        "  3. For a fixed value of the sample size $n$:\n",
        "    - Using the function `sample_estimation_error`, mark 100 samples of $A$ on the generated plot (one sample for one realization of a dataset $Z_{1}^{n}$);\n",
        "    - Using the function `sample_Rad`, mark 100 samples of a function optimizing the supremum in the computation of Rademacher complexity (one sample for one realization of $X_{1}^{n}, \\Omega_{1}^{n}$).\n",
        "  4. Repeat step $3$ for different sample sizes $n$. What do you observe?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1-OiiLvfm2_"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgXc3agRw5WZ"
      },
      "source": [
        "def get_figure_with_shaded_disc():\n",
        "  \"\"\" Returns a figure with a plotted disc of radius 1. \"\"\"\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.set_size_inches(6, 6)\n",
        "  ax.set_ylim(-1.1, 1.1)\n",
        "  ax.set_xlim(-1.1, 1.1)\n",
        "  \n",
        "  # Mark the function space \\mathcal{A} by a gray disc.\n",
        "  function_space = plt.Circle((0,0), 1.0, color='gray', alpha=0.25)\n",
        "  ax.add_patch(function_space)\n",
        "\n",
        "  return fig, ax\n",
        "\n",
        "def scatter_plot_random_points(ax, n_points, random_functions, colors, labels):\n",
        "  \"\"\" A helper function for step 3 of Exercise 3.\n",
        "      :ax: Axis object for plotting.\n",
        "      :n_points: Number of randomly drawn points to plot.\n",
        "      :random_function: A list of random functions, each taking no arguments\n",
        "        and returning a randomly generated point from some distribution.\n",
        "      :colors: A list of colors, one color for each random function.\n",
        "      :labels: A list of labels to be displayed in the legend, one for each\n",
        "        random function.\n",
        "  \"\"\" \n",
        "  point_handles = []\n",
        "  for (random_function, color) in zip(random_functions, colors):\n",
        "    for _ in range(n_points):\n",
        "      point = random_function()\n",
        "      point_handle = ax.scatter(point[0], point[1], color=color, alpha=0.5)\n",
        "    point_handles.append(point_handle)\n",
        "  ax.legend(point_handles, labels)\n",
        "\n",
        "for n in [100, 400, 800, 1600]:\n",
        "  fig, ax = get_figure_with_shaded_disc()\n",
        "  sample_A_lambda = lambda : sample_estimation_error(n, 2)[0]\n",
        "  sample_Rad_lambda = lambda : sample_Rad(n, 2)[0]\n",
        "  scatter_plot_random_points(ax=ax, n_points=100,\n",
        "                            random_functions=[sample_A_lambda,\n",
        "                                              sample_Rad_lambda],\n",
        "                            colors=['C0','C1'],\n",
        "                            labels=['sample of A',\n",
        "                                    'sample of points optimizing Rad'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5cYc37A81Us"
      },
      "source": [
        "In the plots generated above, we observe that as $n$ increases, the randomly drawn empirical risk minimizers $A$ tend to concentrate closer and closer to the population risk minimizer $w^{\\star}$. <font color='green'>**Thus, the effective search space of the empirical risk minimization algorithm decreases as the sample size increases**</font>, because functions far away from the population risk minimizer become less likely to be empirical risk minimizers.\n",
        "\n",
        "On the other hand, the supremum inside the definition of Rademacher complexity is always attained by some function on the boundary of $\\mathcal{A}$. <font color='green'>**Thus,\n",
        "Rademacher complexity does not take into account the fact that the output of empirical risk minimization algorithm gets better with increasing sample size $n$**</font>, which in turn leads to suboptimally scaling upper bounds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASYH4KzmfHKm"
      },
      "source": [
        "## Revisiting the Proof of Rademacher Complexity Upper Bound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syO5c-3B-HdD"
      },
      "source": [
        "In order to fix the problem depicted in the simulations of Exercise 3, let us first revisit the steps taken to arrive at the Rademacher complexity upper bound on the expected estimation error. Below, we assume that the population risk minimizer among $\\mathcal{A}$ exists and is denoted by $a^{\\star}$. We proceed as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eINcQshPQYa-"
      },
      "source": [
        "\\begin{align*}\n",
        "  &\\mathbf{E}_{Z_{1}^{n}}\\left[r(A) - r(a^{\\star})\\right]\n",
        "  \\\\\n",
        "  &=\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - R(A)\n",
        "    + R(A) - R(a^{\\star})\n",
        "    + R(a^{\\star}) - r(a^{\\star})\n",
        "  \\right]\n",
        "  \\\\\n",
        "  &=\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - R(A)\n",
        "    \\right]\n",
        "    + \n",
        "    \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "      R(A) - R(a^{\\star})\n",
        "    \\right]\n",
        "    + \\underbrace{\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "      R(a^{\\star}) - r(a^{\\star})\n",
        "   \\right]}_{=0}\n",
        "  \\\\\n",
        "  &=\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - R(A)\n",
        "    \\right]\n",
        "    + \n",
        "    \\underbrace{\n",
        "    \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "      R(A) - R(a^{\\star})\n",
        "    \\right]\n",
        "    }_{\\leq 0 \\text{ since }A\\text{ is an ERM}}\n",
        "  \\\\\n",
        "  &\\leq\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - R(A)\n",
        "    \\right]\n",
        "  \\\\\n",
        "  &\\leq\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      r(a) - R(a)\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  \\\\\n",
        "  &=\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      r(a) - R(a)\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  \\\\\n",
        "  &=\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      \\mathbf{E}_{(Z')_{1}^{n}}[R'(a)] - R(a)\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  &\\text{introducing an independent copy }(Z')_{1}^{n}\\text{ of }Z_{1}^{n}\n",
        "  \\text{ and letting }R'(a) = \\frac{1}{n}\\sum_{i=1}^{n}\\ell(a, Z_{i}')\n",
        "  \\\\\n",
        "  &\\leq \\mathbf{E}_{Z_{1}^{n}, (Z')_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      R'(a) - R(a)\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  \\\\\n",
        "  &= \\mathbf{E}_{Z_{1}^{n}, (Z')_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      \\frac{1}{n}\\sum_{i=1}^{n}(\\ell(a, Z'_{i}) - \\ell(a, Z_{i}))\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  \\\\\n",
        "  &= \\mathbf{E}_{Z_{1}^{n}, (Z')_{1}^{n}, \\Omega_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      \\frac{1}{n}\\sum_{i=1}^{n}\\Omega_{i}(\\ell(a, Z'_{i}) - \\ell(a, Z_{i}))\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  &\\text{by symmetry}\n",
        "  \\\\\n",
        "  &\\leq \\mathbf{E}_{Z_{1}^{n}, (Z')_{1}^{n}, \\Omega_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      \\frac{1}{n}\\sum_{i=1}^{n}\\Omega_{i}\\ell(a, Z'_{i})\n",
        "    \\right\\}\n",
        "    + \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      \\frac{1}{n}\\sum_{i=1}^{n}-\\Omega_{i} \\ell(a, Z_{i})\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  \\\\\n",
        "  &= 2\\mathbf{E}_{Z_{1}^{n}, \\Omega_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "      \\frac{1}{n}\\sum_{i=1}^{n} \\Omega_{i} \\ell(a, Z_{i})\n",
        "    \\right\\}\n",
        "    \\right]\n",
        "  \\\\\n",
        "  &= 2\\mathbf{E}_{Z_{1}^{n}}\\bigg[\n",
        "      \\underbrace{\n",
        "        \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "        \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "          \\frac{1}{n}\\sum_{i=1}^{n} \\Omega_{i} \\ell(a, Z_{i})\n",
        "        \\right\\}\n",
        "        \\bigg\\vert\n",
        "        Z_{1},\\dots,Z_{n}\n",
        "        \\right]\n",
        "      }_{\n",
        "       = \\mathrm{Rad}(\\mathcal{L} \\circ \\{Z_{1}, \\dots, Z_{n}\\})\n",
        "       = \\mathrm{Rad}(\\{\n",
        "           (\\ell(a, Z_{1}), \\dots, \\ell(a, Z_{n}))^{\\mathsf{T}}\n",
        "           \\in \\mathbb{R}^{n}\n",
        "           : a \\in \\mathcal{A}\n",
        "         \\})\n",
        "      \\text{ by definition}\n",
        "      }\n",
        "    \\bigg]\n",
        "  \\\\\n",
        "  &= 2 \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    \\mathrm{Rad}(\\mathcal{L} \\circ \\{Z_{1}, \\dots, Z_{n}\\}) \n",
        "    \\big\\vert\n",
        "    Z_{1},\\dots,Z_{n}\n",
        "  \\right]\n",
        "  \\\\\n",
        "  &\\leq 2\\gamma \\mathbf{E}_{X_{1}^{n}}\\left[\n",
        "    \\mathrm{Rad}(\\mathcal{A} \\circ \\{X_{1}, \\dots, X_{n}\\}) \n",
        "    \\big\\vert\n",
        "    X_{1},\\dots,X_{n}\n",
        "  \\right]\n",
        "  &\\text{by Ledoux-Talagrand contraction principle for }\\gamma\\text{-Lipschitz losses,}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HzdWRXRj-uU"
      },
      "source": [
        "<font color='green'>**Notice that the key steps in the above proof are symmetrization (introducing the Rademacher random variables) and contraction. It turns out that a refined offset Rademacher complexity upper bound, which will be introduced in the next section, can be obtained via a more careful application of the same techniques.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOqF-RlW_uE2"
      },
      "source": [
        "### Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD9lt2C6-o9P"
      },
      "source": [
        "Consider the linear regression setup described at the beginning of this notebook. When the above chain of derivations is applied to such setting,\n",
        "our previously performed simulations suggest that the left-hand side (i.e., the expected estimation error) is of order $O(1/n)$, while the right-hand side (i.e., the Radmacher complexity upper bound) is of order $\\Omega(1/\\sqrt{n})$. \n",
        "\n",
        "By way of a simulation or otherwise, find the first inequality/suboptimal step that \"loses\" the rate (i.e., the first inequality that upper bounds a term of order $1/n$ by a term of order $1/\\sqrt{n}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVTJz2NYfkaY"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEXT6PBQBIcL"
      },
      "source": [
        "The second inequality below already leads to a suboptimally scaling upper bound of order $\\Omega(1/\\sqrt{n})$\n",
        "\\begin{align*}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[r(A) - r(a^{\\star})\\right]\n",
        "  &\\leq\\mathbf{E}_{Z_{1}^{n}}\\big[\n",
        "    r(A) - R(A)\n",
        "  \\big]\n",
        "  \\\\\n",
        "  &\\leq \\mathbf{E}_{Z_{1}^{n}}\\big[\n",
        "    \\sup_{a \\in \\mathcal{A}} \\left\\{r(a) - R(a)\\right\\}\n",
        "  \\big].\n",
        "\\end{align*}\n",
        "We empirically verify this claim by running the simulations below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEkPEgU0Bxie"
      },
      "source": [
        "def sample_first_upper_bound(n, d):\n",
        "  \"\"\" Returns a 1-Sample Monte Carlo estimate of E[r(A) - R(A)]. \"\"\"\n",
        "  X, y = generate_dataset(n, d)\n",
        "  reg = LinearRegression(fit_intercept=False)\n",
        "  reg.fit(X, y)\n",
        "  A = reg.coef_.reshape(-1, 1)\n",
        "  # Compute the population risk r(A).\n",
        "  # r(A) = ||A - w*||_{2}^{2} + Variance(noise).\n",
        "  w_star = get_w_star(d)\n",
        "  r_A = np.average((A - w_star)**2) + 1.0/3.0\n",
        "  # Compute the empirical risk R(A).\n",
        "  R_A = np.average((X @ A - y)**2)\n",
        "  return r_A - R_A\n",
        "\n",
        "\n",
        "def sample_second_upper_bound(n, d):\n",
        "  \"\"\" Returns a **lower bound** on a 1-Sample Monte Carlo estimate of\n",
        "      E[\\sup_{a \\in \\mathcal{A}} {r(a) - R(a)}]. \"\"\"\n",
        "  # Instead of computing the supremum over \\mathcal{A}, we will generate\n",
        "  # some random vectors w on the unit sphere and compute the maximum over\n",
        "  # our random draws. This will serve as a lower bound for the supremum term.\n",
        "  X, y = generate_dataset(n, d)\n",
        "  supremum_term = 0 # Initialize the supremum term to 0, attained by a = 0.\n",
        "\n",
        "  n_samples = 30\n",
        "  w_star = get_w_star(d)\n",
        "  for _ in range(n_samples):\n",
        "    # Generate a random vector on a unit sphere.\n",
        "    a = sample_from_P_X(n=1, d=d).reshape(-1, 1)\n",
        "    # Compute generalization error r(a) - R(a).\n",
        "    # Note that r(a) = ||a - w*||_{2}^{2} + Variance(noise).\n",
        "    r_a = np.average((a - w_star)**2) + 1.0/3.0\n",
        "    R_a = np.average((X @ a - y)**2)\n",
        "    generalization_error = r_a - R_a\n",
        "    if generalization_error > supremum_term:\n",
        "      # Update the supremum estimate.\n",
        "      supremum_term = generalization_error\n",
        "\n",
        "  return supremum_term\n",
        "\n",
        "\n",
        "# We can now compute Monte Carlo estimates of the upper bounds obtained\n",
        "# via the first two inequalities.\n",
        "d = 10\n",
        "n_monte_carlo_samples = 2500\n",
        "first_step_estimates = np.zeros(len(ns))\n",
        "second_step_estimates = np.zeros(len(ns))\n",
        "\n",
        "np.random.seed(0)\n",
        "for idx, n in enumerate(ns):\n",
        "  for _ in range(n_monte_carlo_samples):\n",
        "    first_step_estimates[idx] += \\\n",
        "      sample_first_upper_bound(n, d) / n_monte_carlo_samples\n",
        "    second_step_estimates[idx] += \\\n",
        "      sample_second_upper_bound(n, d) / n_monte_carlo_samples\n",
        "\n",
        "plt.plot(np.log(ns), np.log(first_step_estimates))\n",
        "plt.plot(np.log(ns), np.log(second_step_estimates))\n",
        "plt.xlabel(r'$\\log n$')\n",
        "plt.legend(['first upper bound', 'second upper bound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ub5toKhfUqa"
      },
      "source": [
        "## Localization via Offset Rademacher Complexities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KxxkCO3FfRj"
      },
      "source": [
        "Recall that all of the below is stated in force of the setup described in the introduction (specifically, in what follows, it is important that the loss $\\ell$ is quadratic and the function class $\\mathcal{A}$ is convex).\n",
        "\n",
        "In Exercise 4, we have found that in the development of Rademacher complexity upper bound on expected estimation error, the step that upper bounds $r(A) - R(A)$ by $\\sup_{a \\in \\mathcal{A}} \\{r(a) - R(a)\\}$ results in suboptimal upper bounds. We have also visualized in Exercise 3 that the Rademacher complexity upper bounds on the above supremum are always attained at the boundary of the set $\\mathcal{A}$, thus ignoring the fact that empirical risk minimization gets better with increasing sample size. <font color='green'>**In this section, we will confront this issue directly by introducing additional terms inside the supremum that will force the maximizers to stay close to the optimal function $a^{\\star}$.**</font>\n",
        "\n",
        "The localization idea that we will now introduce is due to *Liang, Rakhlin and Sridharan [2015]*, however, the more general idea of localization has been known long before; see the bibliographic remarks section for further details.\n",
        "We begin our analysis as before, via the following identity:\n",
        "$$\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[r(A) - r(a^{\\star})\\right]\n",
        "  =\\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - R(A)\n",
        "    + R(A) - R(a^{\\star})\n",
        "    + R(a^{\\star}) - r(a^{\\star})\n",
        "  \\right]. \\tag{1}\n",
        "$$\n",
        "<font color='green'>**This time, however, instead of using the upper bound $R(A) - R(a^{\\star}) \\leq 0$, which follows from the fact that $A$ minimizes the empirical risk, we will\n",
        "use the refined upper bound**</font>\n",
        "$$\n",
        "  R(A) - R(a^{\\star}) \\leq -\\|A - a^{\\star}\\|_{n}^{2}\n",
        "  = -\\frac{1}{n}\\sum_{i=1}^{n} \\left(A(X_{i}) - a^{\\star}(X_{i})\\right)^{2}.\n",
        "$$\n",
        "We omit the proof of the above bound, which follows from the fact that \n",
        "the quadratic loss function as well as the function class $\\mathcal{A}$ is convex. Combining the above bound with equation $(1)$ we have\n",
        "\\begin{align*}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[r(A) - r(a^{\\star})\\right]\n",
        "  &\\leq \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    r(A) - r(a^{\\star}) - (R(A) - R(a^{\\star}))\n",
        "    - \\|A - a^{\\star}\\|_{n}^{2}\n",
        "  \\right]\n",
        "  \\\\\n",
        "  &\\leq \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "    \\sup_{a \\in \\mathcal{A}} \\left\\{\n",
        "    r(a) - r(a^{\\star}) - (R(a) - R(a^{\\star}))\n",
        "    - \\|a - a^{\\star}\\|_{n}^{2}\n",
        "    \\right\\}\n",
        "  \\right]. \\tag{2}\n",
        "\\end{align*}\n",
        "<font color='green'>**The crucial difference with the previously obtained bounds is that in the above expression, the negative term $-\\|a - a^{\\star}\\|_{n}^{2}$ will force the supremum to be attained at some function that is close to $a^{\\star}$, thus avoiding the behaviour observed in Exercise 3.**</font>\n",
        "\n",
        "In a similar manner that we have obtained the Rademacher complexity upper bound on the estimation error in the previous section, using the symmetrization and contraction techniques upper bound $(2)$ can be further upper bounded, for some constants $c,c' > 0$ depending only on boundedness properties of $\\mathcal{Y}$ and $\\mathcal{A}$ and in independent of the sample size $n$:\n",
        "\\begin{align*}\n",
        "  &c'\\mathbf{E}_{X_{1}^{n}}\\bigg[\n",
        "   \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "     \\sup_{a \\in \\mathcal{A}}\\left\\{\n",
        "          \\frac{1}{n} \\sum_{i=1}^{n} \\Omega_{i}(a(X_{i}) - a^{\\star}(X_{i}))\n",
        "          - c\\|a - a^{\\star}\\|_{n}^{2}\n",
        "       \\right\\}\n",
        "     \\bigg\\vert X_{1}, \\dots, X_{n} \n",
        "    \\right]\n",
        "  \\bigg]\n",
        "  \\\\\n",
        "  &=c'\\mathbf{E}_{X_{1}^{n}}\\bigg[\n",
        "   \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "     \\sup_{a \\in \\mathcal{A} - a^{\\star}}\\left\\{\n",
        "          \\frac{1}{n} \\sum_{i=1}^{n} \\Omega_{i}a(X_{i})\n",
        "          - c\\|a\\|_{n}^{2}\n",
        "       \\right\\}\n",
        "     \\bigg\\vert X_{1}, \\dots, X_{n} \n",
        "    \\right]\n",
        "  \\bigg]\n",
        "  &\\text{where }\\mathcal{A} - a^{\\star} = \\{a - a^{\\star} : a \\in \\mathcal{A}\\}.\n",
        "\\end{align*}\n",
        "We will call the inner expectation the *empirical offset Rademacher complexity*:\n",
        "$$\n",
        "  \\mathrm{OffsetRad}(c, (\\mathcal{A} - a^{\\star}) \\circ \\{x_{1}, \\dots, x_{n}\\})\n",
        "  =\n",
        "   \\mathbf{E}_{\\Omega_{1}^{n}}\\left[\n",
        "     \\sup_{a \\in \\mathcal{A} - a^{\\star}}\\left\\{\n",
        "          \\frac{1}{n} \\sum_{i=1}^{n} \\Omega_{i}a(X_{i})\n",
        "          - c\\|a\\|_{n}^{2}\n",
        "       \\right\\}\n",
        "    \\right].\n",
        "$$\n",
        "Putting everything together we arrive at the following upper bound on the expected estimation error:\n",
        "$$\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[r(A) - r(a^{\\star})\\right]\n",
        "  \\leq c' \\mathbf{E}_{X_{1}^{n}}\\left[\n",
        "    \\mathrm{OffsetRad}(c, (\\mathcal{A} - a^{\\star}) \\circ \\{X_{1}, \\dots, X_{n}\\})\n",
        "    \\big\\vert\n",
        "    X_{1},\\dots,X_{n}\n",
        "  \\right].\n",
        "$$\n",
        "The omitted proof details can be found in the proof of Theorem 4 in the paper by [Liang, Rakhlin and Sridharan [2015]](http://proceedings.mlr.press/v40/Liang15.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ekftw75_wLu"
      },
      "source": [
        "### Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L_1jCxnNLcH"
      },
      "source": [
        "Let $\\mathcal{A}_{\\mathbb{R}^{d}} = \\{\\langle w, \\cdot \\rangle : w \\in \\mathbb{R}^{d}\\}$. Notice that since $\\mathcal{A} \\subseteq \\mathcal{A}_{\\mathbb{R}^{d}}$, for any $x_{1}, \\dots, x_{n} \\in \\mathcal{X}$ we have\n",
        "$$\n",
        "      \\mathrm{OffsetRad}(c, (\\mathcal{A} - a^{\\star}) \\circ \\{x_{1}, \\dots, x_{n}\\})\n",
        "      \\leq \\mathrm{OffsetRad}(c, (\\mathcal{A}_{\\mathbb{R}^{d}} - a^{\\star}) \\circ \\{x_{1}, \\dots, x_{n}\\}).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkC7Wwd3emr8"
      },
      "source": [
        "- Fill in the implementation details of the function `sample_OffsetRad` defined in the next cell.\n",
        "- Fix any $c > 0$. Using the function `sample_OffsetRad`, verify empirically that the offset Rademacher complexity\n",
        "   $$\n",
        "      \\mathbf{E}_{X_{1}^{n}}\\left[\n",
        "      \\mathrm{OffsetRad}(c, (\\mathcal{A}_{\\mathbb{R}^{d}} - a^{\\star}) \\circ \\{X_{1}, \\dots, X_{n}\\})\n",
        "      \\big\\vert\n",
        "      X_{1},\\dots,X_{n}\n",
        "    \\right]\n",
        "   $$\n",
        "   decays as $O(1/n)$.\n",
        "\n",
        "   *Remark.* Within our problem setup, the offset Rademacher complexity scales as $O(d/n)$, but you are only asked to focus on the dependence with respect to the sample size $n$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bem2hwBcKt8m"
      },
      "source": [
        "def sample_OffsetRad(n, d, c):\n",
        "  \"\"\" An offset Rademacher complexity equivalent of the function sample_Rad.\n",
        "      :n: Number of data points.\n",
        "      :d: Dimension of the covariates.\n",
        "      :c: A non-negative offset Rademacher complexity parameter.\n",
        "      :returns: A tuple (w_opt, rad), where:\n",
        "        - w_opt is a d-dimensional vector maximizing\n",
        "          \\frac{1}{n}\\sum_{i=1}^{n} (Omega_i * <w - w*, X_i> - c<w - w*, X_i>^2)\n",
        "          with w ranging over all of R^{d};\n",
        "        - offset_rad is a 1-sample Monte Carlo estimate of\n",
        "          E[OffsetRad(c, (A_{R^d} - a*) \\circ {X_1, \\dots, X_n})].\n",
        "  \"\"\"\n",
        "  X = sample_from_P_X(n, d)\n",
        "  Omega = sample_Omegas(n)\n",
        "  w_star = np.ones(d) * 0.5 / np.sqrt(d)\n",
        "  ##############################################################################\n",
        "  # Exercise 5. Fill in the missing implementation details below.\n",
        "  \n",
        "  ##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fitfvhxAfilg"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6LvnILVPRbC"
      },
      "source": [
        "In the implementation of the function `sample_OffsetRad` we are asked to maximize $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ defined as\n",
        "$$\n",
        "  f(w) = \\left\\langle w, \\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}\\Omega_{i}X_{i}}_{= b}\\right\\rangle\n",
        "          - c(w - w^{\\star})^{\\mathsf{T}}\n",
        "          \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}X_{i}^{\\mathsf{T}}\\right)}_{= \\widehat{\\Sigma}}(w - w^{\\star})\n",
        "$$\n",
        "over $w \\in \\mathbb{R}^{d}$. Differentiating with respect to $w$, we find that the optimal value is attained at (assuming invertibility of $\\widehat{\\Sigma}$):\n",
        "$$\n",
        "  w_{\\mathrm{opt}} = w^{\\star} + \\frac{1}{2c}\\widehat{\\Sigma}^{-1}b.\n",
        "$$\n",
        "Plugging in $w_{\\mathrm{opt}}$ into $f$ we obtain\n",
        "$$\n",
        "  \\sup_{w \\in \\mathbb{R}^{d}} f(w) = f(w_{\\mathrm{opt}}) = \\frac{1}{4c}b^{\\mathsf{T}}\\widehat{\\Sigma}^{-1}b.\n",
        "$$\n",
        "Using the above computations, the function `sample_OffsetRad` can be implemented as follows:\n",
        "```\n",
        "def sample_OffsetRad(n, d, c):\n",
        "  X = sample_from_P_X(n, d)\n",
        "  Omega = sample_Omegas(n)\n",
        "  w_star = np.ones(d) * 0.5 / np.sqrt(d)\n",
        "  b = np.average(X * Omega.reshape(-1, 1), axis=0).reshape(-1, 1)\n",
        "  Sigma = np.transpose(X) @ X / n\n",
        "  Sigma_inverse = np.linalg.inv(Sigma)\n",
        "  w_opt = w_star + Sigma_inverse @ b / (2.0 * c)\n",
        "  offset_rad = np.transpose(b) @ Sigma_inverse @ b / (4.0 * c) \n",
        "  return w_opt, offset_rad\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X6QdQ8xTXht"
      },
      "source": [
        "We can now verify that the offset Rademacher complexity decays as $O(1/n)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvHhjoZ8SrUJ"
      },
      "source": [
        "ns = np.arange(start=100, stop=201, step=10)\n",
        "d = 10\n",
        "n_monte_carlo_samples = 2500\n",
        "offset_rad_estimates = np.zeros(len(ns))\n",
        "\n",
        "np.random.seed(0)\n",
        "for idx, n in enumerate(ns):\n",
        "  for _ in range(n_monte_carlo_samples):\n",
        "    offset_rad_estimates[idx] += \\\n",
        "      sample_OffsetRad(n, d, 1)[1] / n_monte_carlo_samples\n",
        "\n",
        "plt.plot(np.log(ns), np.log(offset_rad_estimates))\n",
        "plt.xlabel(r'$\\log n$')\n",
        "plt.ylabel(r'$\\log \\mathbf{E} \\mathrm{OffsetRad}(c, (\\mathcal{A}_{\\mathbb{R}^{d}} - a^{\\star}) \\circ \\{X_{1}, \\dots, X_{n}\\})$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxdhtpxe_zqs"
      },
      "source": [
        "### Exercise 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Aj06UbBevIK"
      },
      "source": [
        "Repeat the simulations performed in Exercise 3, this time with Rademacher complexity replaced by the offset Rademacher complexity defined above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYp3_wSDfhDu"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-aTTkaTUsC6"
      },
      "source": [
        "for n in [100, 400, 800, 1600]:\n",
        "  fig, ax = get_figure_with_shaded_disc()\n",
        "  sample_A_lambda = lambda : sample_estimation_error(n, 2)[0]\n",
        "  sample_OffsetRad_lambda = lambda : sample_OffsetRad(n, 2, c=1.0)[0]\n",
        "  scatter_plot_random_points(ax=ax, n_points=100,\n",
        "                            random_functions=[sample_A_lambda,\n",
        "                                              sample_OffsetRad_lambda],\n",
        "                            colors=['C0','C1'],\n",
        "                            labels=['sample of A',\n",
        "                                    'sample of points optimizing OffsetRad'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uEsfEYgU7sO"
      },
      "source": [
        "<font color='green'>**Observe that this time, in contrast to the results observed in Exercise 3, the effective domain of the sample points optimizing offset Rademacher complexity is decreasing with the increasing sample size $n$. This depicts the desired localization phenomenon.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWuEtjVSdyVH"
      },
      "source": [
        "## Bibliographic Remarks\n",
        "\n",
        "In this practical session we have explored an example where Rademacher complexity measures without localization are insufficient to obtain correct bounds on the estimation error. In fact, it can be shown that global Rademacher complexities cannot yield bounds that decay faster than $1/\\sqrt{n}$;\n",
        "see, for example, *Bartlett and Mendelson [2006, Theorem 2.3]*.\n",
        "\n",
        "Statistical performance of empirical risk minimization algorithms has been studied extensively from the empirical processes theory point of view, where the idea of localization is analogous to the fact that the correct rate of convergence is determined by a fixed point of the continuity modulus of the corresponding empirical process, rather than its supremum. For this point of view see *van de Geer [2000], Massart [2000]* and the references therein.\n",
        "Localizing Rademacher complexities via an approach based on computing fixed points was developed, among other authors, by *Koltchinskii [2001], Bartlett, Bousquet, and Mendelson [2005], Koltchinskii [2006]*. See also the book by *Koltchinskii [2011]*.\n",
        "\n",
        "When learning with the quadratic loss, the idea of localization via shifted empirical processes presented in this practical session was introduced by *Liang, Rakhlin, and Sridharan [2015]*. See also the work of *Zhivotovskiy and Hanneke [2018]* for related ideas applied to the setting of zero-one loss. Recent literature has also focused on obtaining complexity measures that can\n",
        "be used in heavy-tailed scenarios; see *Mendelson [2015, 2020]*.\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "P. L. Bartlett and S. Mendelson. Empirical minimization. Probability theory and related fields, 135(3):311–334, 2006.\n",
        "\n",
        "P. L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals of Statistics, 33(4):1497–1537, 2005.\n",
        "\n",
        "V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47(5):1902–1914, 2001.\n",
        "\n",
        "V. Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 34(6):2593–2656, 2006. \n",
        "\n",
        "V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole d’Eté de Probabilités de Saint-Flour XXXVIII-2008, volume 2033. Springer Science & Business Media, 2011.\n",
        "\n",
        "T. Liang, A. Rakhlin, and K. Sridharan. Learning with square loss: Localization through offset rademacher complexity. In Conference on Learning Theory, pages 1260–1285, 2015.\n",
        "\n",
        "P. Massart. Some applications of concentration inequalities to statistics. In Annales de la Faculté des sciences de Toulouse: Mathématiques, volume 9, pages 245–303, 2000.\n",
        "\n",
        "S. Mendelson. Learning without concentration. J. ACM, 62(3), June 2015. ISSN 0004-5411. doi: 10.1145/2699439. URL https://doi.org/10.1145/2699439.\n",
        "\n",
        "S. Mendelson. Extending the scope of the small-ball method. Studia Mathematica, pages 1–21, 2020.\n",
        "\n",
        "S. van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.\n",
        "\n",
        "N. Zhivotovskiy and S. Hanneke. Localization of vc classes: Beyond local rademacher complexities. Theoretical Computer Science, 742:27–49, 2018\n"
      ]
    }
  ]
}