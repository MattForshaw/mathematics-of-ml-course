{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implicit_regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Oj7xECbGAR"
      },
      "source": [
        "# Implicit/Iterative/Early-Stopping Regularization\n",
        "\n",
        "<font color='green'> In this practical session, we introduce the concept of implicit (also called iterative or early-stopping) regularization. Our main objectives are the following:\n",
        "</font>\n",
        "- <font color='green'>understanding connections between ridge regression regularization and iterative regularization via gradient descent;</font>\n",
        "- <font color='green'>introducing an algorithm with sparsity-inducing implicit regularization effect;</font>\n",
        "- <font color='green'>understanding how the choice of an optimization algorithm can impact the statistical properties of the traced optimization path.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx-bim_Ela8o"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.experimental.numpy as tnp \n",
        "tnp.experimental_enable_numpy_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSiohvOI3-l0"
      },
      "source": [
        "## Reusing Code From the \"Optimization\" Practical Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s54JvdCI9Dz_"
      },
      "source": [
        "We begin this session by importing some of the code already used in the \"optimization\" practical session. First, we will reuse our code for running gradient descent simulations by importing the `Optimizer` and `GradientDescent` classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF-r-qpt9DQr"
      },
      "source": [
        "class Optimizer(object):\n",
        "  \"\"\" A base class for optimizers. \"\"\"\n",
        "\n",
        "  def __init__(self, eta):\n",
        "    \"\"\" :eta_t: A function taking as argument the current iteration t and\n",
        "          returning the step size eta_t to be used in the current iteration. \"\"\"\n",
        "    super().__init__()\n",
        "    self.eta = eta\n",
        "    self.t = 0 # Set iterations counter.\n",
        "\n",
        "  def apply_gradient(self, x_t, g_t):\n",
        "    \"\"\" Given the current iterate x_t and gradient g_t, updates the value\n",
        "      of x_t to x_(t+1) by performing one iterative update.\n",
        "        :x_t: A tf.Variable which value is to be updated.\n",
        "        :g_t: The gradient value, to be used for performing the update.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"To be implemented by subclasses.\")\n",
        "\n",
        "  def step(self, f, x_t):\n",
        "    \"\"\" Updates the variable x_t by performing one optimization iteration.\n",
        "        :f: A function which is being minimized.\n",
        "        :x_t: A tf.Variable with respect to which the function is being\n",
        "          minimized and which value is to be updated\n",
        ".\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      fx = f(x_t)\n",
        "    g_t = tape.gradient(fx, x_t)\n",
        "    self.apply_gradient(x_t, g_t)\n",
        "    # Update the iterations counter.\n",
        "    self.t += 1\n",
        "\n",
        "  def optimize(self, f, x_t, n_iterations):\n",
        "    \"\"\" Applies the function step n_iterations number of times, starting from\n",
        "      the iterate x_t. Note: the number of iterations member self.t is not\n",
        "      restarted to 0, which may affects the computed step sizes. \n",
        "        :f: Function to optimize.\n",
        "        :x_t: Current iterate x_t.\n",
        "        :returns: A list of length n_iterations+1, containing the iterates\n",
        "          [x_t, x_{t+1}, ..., x_{t+n_iterations}].\n",
        "    \"\"\"\n",
        "    x = tf.Variable(x_t)\n",
        "    iterates = []\n",
        "    iterates.append(x.numpy().reshape(-1,1))\n",
        "    for _ in range(n_iterations):\n",
        "      self.step(f, x)\n",
        "      iterates.append(x.numpy().reshape(-1,1))\n",
        "    return iterates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea7pfJk6AiHh"
      },
      "source": [
        "class GradientDescent(Optimizer):\n",
        "  \"\"\" An implementation of gradient descent uptades. \"\"\"\n",
        "\n",
        "  def apply_gradient(self, x_t, g_t):\n",
        "    eta_t = self.eta(self.t)\n",
        "    x_t.assign_add(-eta_t * g_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQAy4V28pIF"
      },
      "source": [
        "It will be helpful to visualize the solution paths generated by different procedures. We will reuse the class `Convergence2DPlotting` from the \"optimization\" practical session notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2jdnHe98FwZ"
      },
      "source": [
        "class Convergence2DPlotting(object):\n",
        "  \"\"\" Plotting utils for visualizing optimization paths on 2D functions. \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.fig, self.ax = plt.subplots()\n",
        "    self.fig.set_size_inches(8.0, 8.0)\n",
        "    self.ax.set_aspect('equal')\n",
        "\n",
        "  def plot_iterates(self, iterates, color='C0'):\n",
        "    iterates = np.array(iterates).squeeze()\n",
        "    x, y = iterates[:,0], iterates[:, 1]\n",
        "    self.ax.scatter(x,y,s=0)\n",
        "    for i in range(len(x)-1):\n",
        "      self.ax.annotate('', xy=(x[i+1], y[i+1]), xytext=(x[i], y[i]),\n",
        "                       arrowprops={'arrowstyle': '->',\n",
        "                                  'color':  color, 'lw': 2})\n",
        "\n",
        "  def plot_contours(self, f):\n",
        "    x_min, x_max = self.ax.get_xlim()\n",
        "    y_min, y_max = self.ax.get_ylim()\n",
        "\n",
        "    # Generate the contours of f on the above computed range.\n",
        "    n_points = 50\n",
        "    x = np.linspace(start=x_min, stop=x_max, num=n_points)\n",
        "    y = np.linspace(start=y_min, stop=y_max, num=n_points)\n",
        "    x, y = np.meshgrid(x, y)\n",
        "    z = np.zeros_like(x)\n",
        "    for x_idx in range(n_points):\n",
        "      for y_idx in range(n_points):\n",
        "        input = np.array([x[x_idx, y_idx], y[x_idx, y_idx]]).reshape(2,1)\n",
        "        z[x_idx, y_idx] = f(input)\n",
        "    self.ax.contour(x,y,z, colors='k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EazmQxm3AsOt"
      },
      "source": [
        "## Ridge Regression vs Unregularized Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMpfKWJ4e98I"
      },
      "source": [
        "In this section, we attempt to understand the similarities between the ridge regression *regularization paths* and gradient descent *optimization paths* obtained by applying gradient descent updates to the *unregularized* empirical risk. For simplicity, we will consider a simple data-generating mechanism given by\n",
        "\\begin{align*}\n",
        "  X &\\sim N(0, I_{d}), \\\\\n",
        "  Y \\mid X = x &\\sim N(\\langle w^{\\star}, x\\rangle, \\sigma^{2}),\n",
        "\\end{align*}\n",
        "where $w^{\\star} \\in \\mathbb{R}^{d}$ denotes some ground-truth parameter.\n",
        "We implement code for sampling data from such distributions in the following cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4UK3bVgAwOu"
      },
      "source": [
        "class GaussianData(object):\n",
        "  \"\"\" A class for sampling Gaussian data with i.i.d. N(0, I_d) covariates. \"\"\"\n",
        "  \n",
        "  def __init__(self, n, d, w_star, noise_std):\n",
        "    \"\"\" :n: Number of data points.\n",
        "        :d: Dimension of the covariates.\n",
        "        :w_star: A d-dimensional vector used to generate noisy observations\n",
        "          y_i = <w_star, x_i> + N(0, noise_std**2).\n",
        "        :noise_std: Standard deviation of the zero-mean Gaussian label noise.\n",
        "    \"\"\"\n",
        "    self.n = n\n",
        "    self.d = d\n",
        "    self.w_star = w_star.reshape(self.d, 1)\n",
        "    self.noise_std = noise_std\n",
        "    self.resample_data(seed=0)\n",
        "\n",
        "  def resample_data(self, seed):\n",
        "    \"\"\" Resamples dataset X, y using the given seed and stores it as class\n",
        "      members. \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    self.X = np.random.normal(loc=0.0, scale=1.0, size=(self.n, self.d))\n",
        "    self.xi = np.random.normal(loc=0.0, scale=1.0, size=(self.n, 1))\n",
        "    self.y = self.X @ self.w_star + self.xi\n",
        "\n",
        "  def compute_empirical_risk(self, w):\n",
        "    \"\"\" For a d-dimensional vector w, outputs R(w) = 1/n ||Xw - y||_{2}^{2}. \"\"\"\n",
        "    return tnp.average((self.X @ w - self.y)**2)\n",
        "\n",
        "  def compute_population_risk(self, w):\n",
        "    \"\"\" For a d-dimensional vector w, outputs\n",
        "      r(w) = ||w-w*||_{2}^{2} + noise_std**2. \"\"\"\n",
        "    return tnp.sum((w - self.w_star)**2) + self.noise_std**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "turDM8y7iTnd"
      },
      "source": [
        "For the next exercise, fix the following setup. You are encouraged, however, to also experiment with other parameter choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atz28J5_ifeK"
      },
      "source": [
        "isotropic_gaussian_2d_data = GaussianData(\n",
        "    n=15,\n",
        "    d=2, # d=2 for visualizing the paths.\n",
        "    w_star=np.array([1.0, 0.0]), # Make w* \"sparse\".\n",
        "    noise_std=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwlzOJoQJ0aa"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGC67jmnJ378"
      },
      "source": [
        "This exercise introduces some connections between ridge regression and gradient descent applied to unregularized empirical risk. In your simulations, use the `isotropic_gaussian_2d_data` object, which contains  two-dimensional Gaussian dataset $X, y$, where $X \\in \\mathbb{R}^{n \\times 2}$ and $y \\in \\mathbb{R}^{n}$.\n",
        "\n",
        "- For a grid of $m$ regularization parameters $\\lambda_{1}, \\dots, \\lambda_{m}$ of your choice, compute ridge regression estimates $w_{\\lambda}^{\\text{ridge}} = \\mathrm{argmin}_{w \\in \\mathbb{R}^{d}} \\frac{1}{n}\\|Xw - y\\|_{2}^{2} + \\frac{\\lambda}{n}\\|w\\|_{2}^{2}$.\n",
        "\n",
        "- Let $w_{0}^{\\text{gd}} = 0$. For some numer of iterations $T$ of your choice, compute iterates of gradient descent $w^{\\text{gd}}_{t+1} = w^{\\text{gd}}_{t} - \\eta \\nabla R(w_{t}^{\\mathrm{gd}})$, where $R(w) = \\frac{1}{n}\\|Xw - y\\|_{2}^{2}$ is the empirical risk and $\\eta > 0$  is some constant step size (the exact value does not matter, as long as it is small enough).\n",
        "\n",
        "- Plot the population risks attained by ridge regression *regularization path* $(w^{\\text{ridge}}_{\\lambda_{i}})_{i=1}^{m}$ and gradient descent *optimization path* $(w^{\\text{gd}}_{t})_{t=0}^{T}$. Comment on your findings.\n",
        "\n",
        "- Using the class `Convergence2DPlotting` visualize the computed ridge regression regularization path and gradient descent optimization path.\n",
        "\n",
        "- Comment on different computational considerations concerning the computation of regularization and optimization paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-b5TLSLaYJT"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ8bDL2Nj8pV"
      },
      "source": [
        "For a given $\\lambda > 0$, ridge regression estimate $w_{\\lambda}$ can be computed by $(X^{\\mathsf{T}}X + \\lambda I)^{-1} X^{\\mathsf{T}}y$. We implement computation of ridge regression regularization paths below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvcMDjyjlE5j"
      },
      "source": [
        "def compute_ridge_regression_regularization_path(data, lambdas):\n",
        "  \"\"\" :data: An object of type GaussianData.\n",
        "      :lambdas: A sorted list of regularization parameters lambda.\n",
        "      :returns: A list of fitted parameters w_{\\lambda}, one for\n",
        "        each provided lambda.\n",
        "  \"\"\"\n",
        "  # Compute and store the values of X^{t}X and X^{t}y and Id.\n",
        "  XtX = np.transpose(data.X) @ data.X\n",
        "  Xty = np.transpose(data.X) @ data.y\n",
        "  Id = np.identity(data.d)\n",
        "  \n",
        "  regularization_path = []\n",
        "  for l in lambdas:\n",
        "    w_lambda = np.linalg.inv(XtX + l*Id) @ Xty\n",
        "    regularization_path.append(w_lambda)\n",
        "  return regularization_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9oI5_8Xmb5l"
      },
      "source": [
        "# Create a grid of regularization parameters equally spaced on a log scale.\n",
        "lambdas = np.exp(np.linspace(start=np.log(10**(-3)), stop=np.log(1e3),\n",
        "                             num=2000))\n",
        "ridge_regularization_path = compute_ridge_regression_regularization_path(\n",
        "    isotropic_gaussian_2d_data, lambdas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5GblpXPqGZX"
      },
      "source": [
        "Computation of gradient descent optimization path can be readily obtained via `GradientDescent` optimizer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkdbH0KssVMu"
      },
      "source": [
        "T = 2000\n",
        "eta = lambda t: 0.001\n",
        "w_0 = np.zeros(isotropic_gaussian_2d_data.d).reshape(-1, 1)\n",
        "gradient_descent = GradientDescent(eta)\n",
        "gd_optimization_path = gradient_descent.optimize(\n",
        "  isotropic_gaussian_2d_data.compute_empirical_risk, w_0, T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1wIc93XxlN6"
      },
      "source": [
        "We can now compare the population (or out-of-sample) risks attained by ridge regression and gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClxCf52xqM_"
      },
      "source": [
        "ridge_risks = [isotropic_gaussian_2d_data.compute_population_risk(w) \\\n",
        "               for w in ridge_regularization_path]\n",
        "gd_risks = [isotropic_gaussian_2d_data.compute_population_risk(w) \\\n",
        "            for w in gd_optimization_path]\n",
        "plt.plot(ridge_risks)\n",
        "plt.plot(gd_risks)\n",
        "plt.xlabel('Parameter index')\n",
        "plt.ylabel('Population risk')\n",
        "plt.legend(['ridge', 'gd'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAhf5ws66t6i"
      },
      "source": [
        "We observe that the minimum population risk achieved by both algorithms is approximately the same. Notice that large values of $\\lambda$ for ridge regression correspond to small values of $t$ for gradient descent (i.e., using a lot of regularization). On the other hand, small values of $\\lambda$ correspond to large values of $t$ (using little regularization).\n",
        "\n",
        "Let us now plot the obtained regularization and optimization paths against the contours of the population risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW5BJfEf72Nt"
      },
      "source": [
        "path_plots = Convergence2DPlotting()\n",
        "path_plots.plot_iterates(ridge_regularization_path, color='C0')    \n",
        "path_plots.plot_iterates(gd_optimization_path, color='C1')\n",
        "path_plots.ax.set_ylim(-0.2, 0.45)\n",
        "path_plots.ax.set_xlim(-0.1, 1.1)\n",
        "path_plots.plot_contours(isotropic_gaussian_2d_data.compute_population_risk)\n",
        "path_plots.ax.scatter(isotropic_gaussian_2d_data.w_star[0],\n",
        "                      isotropic_gaussian_2d_data.w_star[1],\n",
        "                      marker='x',\n",
        "                      color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lA_Dpfc8kbr"
      },
      "source": [
        "<font color='green'>**We observe that gradient descent optimization path nearly matches that of ridge regression regularization path.**</font> We will make this observation more precise in the next exercise.\n",
        "\n",
        "Finally, regarding the computational considerations, notice that we generated new points along ridge regression optimization problem by solving a new optimization problem for each value of $\\lambda$. While there are better ways to compute regularization paths (e.g., based on *warm restarts*), computational analysis and implementation might become more tricky in such cases. On the other hand, obtaining new iterates along the gradient optimization path only cost one gradient descent update, resulting in arguably simpler and more efficient procedure.\n",
        "\n",
        "<font color='green'>**We observe that the gradient descent optimization path nearly matches that of the ridge regression regularization path.**</font> We will make this observation more precise in the next exercise.\n",
        "\n",
        "Finally, regarding the computational considerations, notice that <font color='green'>**we generated new points along the ridge regularization path by solving a new optimization problem for each value of $\\lambda$.**</font> While there are better ways to compute regularization paths (e.g., based on *warm restarts*), computational analysis and implementation might become more tricky in such cases. On the other hand, <font color='green'>**obtaining new iterates along the gradient optimization path only cost one gradient descent update**</font>, resulting in an arguably more straightforward and more efficient procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ8J8tafazgI"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0daTkG4be_E"
      },
      "source": [
        "In exercse 1, we have observed that the gradient descent optimization path nearly matches the ridge regression regularization path. In this exercise we investigate why this happens. Recall that ridge regression estimates $w_{\\lambda}$ can be computed via the following expression:\n",
        "$$\n",
        "  w_{\\lambda} = T^{\\text{ridge}}_{\\lambda}(X^{\\mathsf{T}}X)X^{\\mathsf{T}}y ,\\text{ where }\n",
        "  T^{\\text{ridge}}_{\\lambda}(X^{\\mathsf{T}}X) = (X^{\\mathsf{T}}X + \\lambda I_{d})^{-1}.\n",
        "$$\n",
        "The mapping $T_{\\lambda}^{\\text{ridge}}$ can be seen as an operator acting on the eigenvalues of the sample covariance matrix $X^{\\mathsf{T}}X$. In particular, $T_{\\lambda}^{\\text{ridge}}$ aims to invert the eigenvalues of $X^{\\mathsf{T}}X$, with the inverse getting closer to the true inverse as $\\lambda \\to 0$.\n",
        "\n",
        "- Find an expression for some operator $T^{\\text{gd}}_{\\eta, t}$ acting on the eigenvalues of $X^{\\mathsf{T}}X$, such that the $t$-th iterate of gradient descent, obtained with a constant step-size $\\eta$ and $w_0 = 0$ satisfies\n",
        "$$\n",
        "  w_{t} = T^{\\text{gd}}_{\\eta, t}(X^{\\mathsf{T}}X)X^{\\mathsf{T}}y.\n",
        "$$\n",
        "- Using the above-computed expression, suggest (informally) some mapping $f$ between $\\lambda$ and $(\\eta, t)$, such that $w^{\\text{ridge}}_{\\lambda} \\approx w^{\\text{gd}}_{t}$.\n",
        "- Using the above-suggested mapping $f$ which approximately corresponds to $\\lambda = f(\\eta, t)$, plot gradient descent and ridge regression population risks computed in Exercise 1. For the x-axis, use the number of gradient descent iterations. The ridge regression lambdas should be renormalized using the mapping $f$. **You should find a mapping $f$ such that the two curves approximately overlap.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TAQAKalD-rB"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qlSlTrXEArR"
      },
      "source": [
        "For gradient descent, it can be shown by induction that\n",
        "$$\n",
        "  w_{t}^{\\text{gd}} = \\underbrace{\\frac{2\\eta}{n}\\left(I + \\left(I - \\frac{2\\eta}{n}X^{\\mathsf{T}}X\\right) + \\dots + \\left(I - \\frac{2\\eta}{n}X^{\\mathsf{T}}X\\right)^{t-1}\\right)}_{= T_{\\eta,t}^{\\text{gd}}(X^{\\mathsf{T}}X) }X^{\\mathsf{T}}y.\n",
        "$$\n",
        "Suppose that the eigenvalues of $X^{\\mathsf{T}}X$ are given by $\\mu_{1}, \\dots, \\mu_{d}$. Then, the operator $T_{\\eta,t}^{\\text{gd}}$ acts on the $i$-th eigenvalue $\\mu_{i}$ via the formula:\n",
        "$$\n",
        "  \\mu_{i} \\mapsto \\frac{2\\eta}{n}\\sum_{s=0}^{t-1}\\left(1 - \\frac{2\\eta}{n}\\mu_{i}\\right)^{s}\n",
        "  =\n",
        "  \\frac{1 - \\left(1 - \\frac{2\\eta}{n}\\mu_{i}\\right)^{t}}{\\mu_{i}}.\n",
        "$$\n",
        "<font color='green'>**In essence, the gradient descent iterates are inverting the eigenvalues of the sample covariance matrix using a slightly different formula than ridge regression.**</font>\n",
        "Recall that the corresponding ridge operator $T_{\\lambda}^{\\text{ridge}}$ acts on the $i$-th eigenvalue $\\mu_{i}$ via the mapping\n",
        "$$\n",
        "  \\mu_{i} \\mapsto \\frac{1}{\\mu_{i} + \\lambda}.\n",
        "$$\n",
        "Equating the right-hand sides of the two expressions above, we for given values of $\\lambda$ and $\\eta$, we want to find $t$ such that\n",
        "$$\n",
        "  \\frac{1 - \\left(1 - \\frac{2\\eta}{n}\\mu_{i}\\right)^{t}}{\\mu_{i}}\n",
        "  \\approx \\frac{1}{\\mu_{i} + \\lambda}.\n",
        "$$\n",
        "In the isotropic setting considered in our simulations, we have $\\mu_{1} \\approx \\mu_{2} \\approx n$. Solving the above equation for $t$ yields the expression\n",
        "$$\n",
        "  t \\approx \\frac{\\log\\frac{\\lambda}{n + \\lambda}}{\\log(1-2\\eta)}.\n",
        "$$\n",
        "We apply the above formula in the below code cell. Observe that the generated risk curves using this mapping are aligned as required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd4CsclHEzTd"
      },
      "source": [
        "# Note that small lambda corresponds to large T (i.e., fitting to convergence).\n",
        "eta = gradient_descent.eta(0)\n",
        "X = isotropic_gaussian_2d_data.X\n",
        "n = X.shape[0]\n",
        "remapped_lambdas = np.log(lambdas/(n +lambdas)) / np.log(1.0 - 2.0*eta)\n",
        "# We want to keep the largest remmaped_lambdas element up to T, so we will\n",
        "# remove some part of this above lambdas.\n",
        "truncate_idx = np.argmin(remapped_lambdas > T)\n",
        "plt.plot(remapped_lambdas[truncate_idx:], ridge_risks[truncate_idx:])\n",
        "plt.plot(np.arange(T+1), gd_risks)\n",
        "plt.xlabel('t')\n",
        "plt.ylabel('Population risk')\n",
        "plt.legend(['ridge (lambda mapped to t)', 'gd'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTiHI9Hatyd"
      },
      "source": [
        "## Sparsity-Inducing Implicit Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xSXkRR5CVqG"
      },
      "source": [
        "In the previous section we have seen how gradient descent applied to the unregularized empirical risk generate iterates that approximately correspond to the ridge regression regularization path. Ridge regularization is, of course, just one of the many ways to induce a regularizing effect. For instance, for learning problems with some underlying sparsity structure, lasso regularization might be preferred, giving the regularization path $(w_{\\lambda}^{\\text{lasso}})_{\\lambda \\geq 0}$ whose iterates are defined by\n",
        "$$\n",
        "  w_{\\lambda} \\in \\mathrm{argmin}_{w \\in \\mathbb{R}^{d}} \\frac{1}{n} \\|Xw - y\\|_{2}^{2} + \\frac{\\lambda}{n}\\|w\\|_{1}.\n",
        "$$\n",
        "Indeed, the toy setup explored in our previous simulations exhibits some underlying sparsity structure, as the parameter that generates the data $w^{\\star} = (1, 0)^{\\mathsf{T}}$ can be considered as a sparse vector.\n",
        " We can indeed show that the lasso regularization path contains marginally better solutions for our two-dimensional toy problem than the ridge regularization path (for larger and more sparse problems lasso penalties would significantly outperform ridge penalties)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV98P7mlGIwO"
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "def compute_lasso_regression_regularization_path(data, lambdas):\n",
        "  regularization_path = []\n",
        "  for l in lambdas:\n",
        "    lasso = linear_model.Lasso(alpha=l/(2.0*n), fit_intercept=False)\n",
        "    lasso.fit(data.X, data.y)\n",
        "    w_lambda = lasso.coef_.reshape(-1, 1)\n",
        "    regularization_path.append(w_lambda)\n",
        "  return regularization_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GLHDPD7HRAO"
      },
      "source": [
        "lasso_regularization_path = compute_lasso_regression_regularization_path(\n",
        "    isotropic_gaussian_2d_data, lambdas)\n",
        "lasso_risks = [isotropic_gaussian_2d_data.compute_population_risk(w) \\\n",
        "               for w in lasso_regularization_path]\n",
        "plt.plot(ridge_risks, color='C0')\n",
        "plt.plot(lasso_risks, color='C2')\n",
        "plt.xlabel(r'$\\lambda$')\n",
        "plt.ylabel('Population risk')\n",
        "plt.legend(['ridge', 'lasso'])\n",
        "plt.ylim(0.9,1.4)\n",
        "plt.show()\n",
        "path_plots = Convergence2DPlotting()\n",
        "path_plots.plot_iterates(ridge_regularization_path, color='C0')    \n",
        "path_plots.plot_iterates(lasso_regularization_path, color='C2')\n",
        "path_plots.ax.set_ylim(-0.2, 0.45)\n",
        "path_plots.ax.set_xlim(-0.1, 1.1)\n",
        "path_plots.plot_contours(isotropic_gaussian_2d_data.compute_population_risk)\n",
        "path_plots.ax.scatter(isotropic_gaussian_2d_data.w_star[0],\n",
        "                      isotropic_gaussian_2d_data.w_star[1],\n",
        "                      marker='x',\n",
        "                      color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MvKDVXoJmM3"
      },
      "source": [
        "A natural question occurs: <font color='green'>**how can one introduce a non-Euclidean regularization effect implicitly?**</font> In the following exercise, we present one such way, which induces a sparsity-promoting regularization effect by running gradient descent on a reparametrized coordinate system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UXZSb6MBVVs"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAZ0FtKcMQ1x"
      },
      "source": [
        "For a vector $u \\in \\mathbb{R}^{d}$ let $u^{2}$ denote a component-wise square operation. Consider the reparametrization $w = u^{2}$ (viable for any $w \\in \\mathbb{R}^{d}$ with non-negative components (to treat the general case, we could instead consider $w = u^{2} - v^{2}$; for simplicity, we will consider the reparametrization $w = u^{2}$ only).\n",
        "\n",
        "In the new coordinate system $w = u^{2}$, the empirical risk function is defined as\n",
        "$$\n",
        "  \\widetilde{R}(u) = \\frac{1}{n} \\|Xu^{2} - y\\|_{2}^{2}.\n",
        "$$\n",
        "Consider setting $u_{0} = (0.01, 0.01)^{\\mathsf{T}}$ and running gradient descent on $\\widetilde{R}$ for the problem instance stored in the python variable `isotropic_gaussian_2d_data`:\n",
        "- plot the population risks traced by the gradient descent iterates $(u_{t})_{t=0}^{T}$;\n",
        "- plot the two-dimensional optimization path;\n",
        "- compare the simulation output with the ridge and lasso regularization paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCiMEauWBYET"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vTM7ynALLjY"
      },
      "source": [
        "The reparametrized gradient descent can be implemented in just a few lines by redefining the function we want to optimize in the new coordinate system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdTztsYkc9BM"
      },
      "source": [
        "class ReparametrizedGradientDescent(GradientDescent):\n",
        "\n",
        "  def optimize(self, f, w_0, n_iterations):\n",
        "    \"\"\" The initialization point w_0 needs to have stricly positive coordinates.\n",
        "    This function optimizes f using reparametrization w = u^2, running gradient\n",
        "    descent updates on the parameter u. \"\"\"\n",
        "    def g(u):\n",
        "      return f(u**2)\n",
        "    u_0 = tnp.sqrt(w_0)\n",
        "    u_iterates = super().optimize(g, tnp.sqrt(w_0), n_iterations)\n",
        "    w_iterates = [u**2 for u in u_iterates]\n",
        "    return w_iterates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CaKSrENLdPF"
      },
      "source": [
        "We can now execute the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CenTGjXUAJG"
      },
      "source": [
        "w_0 = np.array([1e-4, 1e-4]).reshape(-1, 1)\n",
        "reparametrized_gd = ReparametrizedGradientDescent(eta = lambda t : 0.005)\n",
        "reparametrized_iterates = reparametrized_gd.optimize(\n",
        "    isotropic_gaussian_2d_data.compute_empirical_risk, w_0, 2000)\n",
        "reparametrized_gd_risks = [isotropic_gaussian_2d_data.compute_population_risk(w) \\\n",
        "               for w in reparametrized_iterates]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY0T8VCyLf0m"
      },
      "source": [
        "Finally, we can visually observe that the reparametrized gradient descent is indeed biased towards first exploring sparse iterates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjt_sAkvUVOe"
      },
      "source": [
        "plt.plot(ridge_risks, color='C0')\n",
        "plt.plot(gd_risks, color='C1')\n",
        "plt.plot(lasso_risks, color='C2')\n",
        "plt.plot(reparametrized_gd_risks, color='C3')\n",
        "plt.xlabel(r'$\\lambda$ or $t$')\n",
        "plt.ylabel('Population risk')\n",
        "plt.legend(['ridge', 'gd', 'lasso', 'reparametrized gd'])\n",
        "plt.ylim(1.0,1.3)\n",
        "plt.show()\n",
        "\n",
        "path_plots = Convergence2DPlotting()\n",
        "path_plots.plot_iterates(ridge_regularization_path, color='C0')    \n",
        "path_plots.plot_iterates(gd_optimization_path, color='C1')\n",
        "path_plots.plot_iterates(lasso_regularization_path, color='C2')\n",
        "path_plots.plot_iterates(reparametrized_iterates, color='C3')\n",
        "path_plots.ax.set_ylim(-0.2, 0.45)\n",
        "path_plots.ax.set_xlim(-0.1, 1.1)\n",
        "path_plots.plot_contours(isotropic_gaussian_2d_data.compute_population_risk)\n",
        "path_plots.ax.scatter(isotropic_gaussian_2d_data.w_star[0],\n",
        "                      isotropic_gaussian_2d_data.w_star[1],\n",
        "                      marker='x',\n",
        "                      color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myOMA-TuERMX"
      },
      "source": [
        "## Bibliographic Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwYJl7FkYzo7"
      },
      "source": [
        "The use of early-stopping as a regularization mechanism dates back at least to Landweber iteration [*Landweber, 1951*] studied in the context of ill-posed inverse problems; for this point of view, see the textbook by *Engl, Hanke, and Neubauer [1996]*. As early as in the 90s, early stopping has also been one of the standard ways to regularize the training procedures used to fit neural network parameters [*Prechelt, 1998*]. In the statistics literature, the first\n",
        "paper to connect early stopping to the notion of minimax optimality is due to *Bühlmann and Yu [2003]*. Implicit regularization effects in the Euclidean setting of gradient descent updates are by now well-understood; see, for example, the paper by *Yao, Rosasco, and Caponnetto [2007]*, *Raskutti, Wainwright, and Yu [2014]*, *Wei, Yang, and Wainwright [2019]*.\n",
        "Much less is known about non-Euclidean setups. The quadratic reparametrization trick was noticed in [*Gunasekar, Woodworth, Bhojanapalli, Neyshabur, and Srebro, 2017*] in the context of matrix factorization. Linear regression setting with quadratic reparametrization was studied by *Vaškevičius, Kanade, and Rebeschini [2019]*, where minimax optimal bounds were obtained for early-stopped iterates under the signal-sparsity and the restricted isometry assumption on the design matrix. Later, it was shown in [*Vaškevičius, Kanade, and Rebeschini, 2020*] how to obtain estimation error upper bounds for early-stopped iterates of mirror descent algorithms via offset Rademacher complexities. For other works concerning early-stopping in non-Euclidean setups, see, for example, [*Osher, Ruan, Xiong, Yao, and Yin, 2016, Molinari,\n",
        "Massias, Rosasco, and Villa, 2021, Wu and Rebeschini, 2021*]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2yYioTPZt52"
      },
      "source": [
        "**References**\n",
        "\n",
        "P. Bühlmann and B. Yu. Boosting with the l2 loss: regression and classification. Journal of the American Statistical Association, 98(462):324–339, 2003.\n",
        "\n",
        "H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems, volume 375. Springer Science & Business Media, 1996.\n",
        "\n",
        "S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pages 6151–6159, 2017.\n",
        "\n",
        "L. Landweber. An iteration formula for fredholm integral equations of the first kind. American journal of mathematics, 73(3):615–624, 1951.\n",
        "\n",
        "C. Molinari, M. Massias, L. Rosasco, and S. Villa. Iterative regularization for convex regularizers. In International Conference on Artificial Intelligence and Statistics, pages 1684–1692. PMLR, 2021.\n",
        "\n",
        "S. Osher, F. Ruan, J. Xiong, Y. Yao, and W. Yin. Sparse recovery via differential inclusions. Applied and Computational Harmonic Analysis, 41(2):436–469, 2016.\n",
        "\n",
        "L. Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55–69. Springer, 1998.\n",
        "\n",
        "G. Raskutti, M. J. Wainwright, and B. Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. The Journal of Machine Learning Research, 15(1):335–366, 2014.\n",
        "\n",
        "T. Vaškevičius, V. Kanade, and P. Rebeschini. The statistical complexity of early-stopped mirror descent. arXiv preprint arXiv:2002.00189, 2020.\n",
        "\n",
        "T. Vaškevičius, V. Kanade, and P. Rebeschini. Implicit regularization for optimal sparse recovery. In Advances in Neural Information Processing Systems, pages 2968–2979, 2019.\n",
        "\n",
        "Y. Wei, F. Yang, and M. J. Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. IEEE Transactions on Information Theory, 65(10):6685–6703, 2019.\n",
        "\n",
        "F. Wu and P. Rebeschini. Nearly minimax-optimal rates for noisy sparse phase retrieval via early-stopped mirror descent. arXiv preprint arXiv:2105.03678, 2021.\n",
        "\n",
        "Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289–315, 2007."
      ]
    }
  ]
}