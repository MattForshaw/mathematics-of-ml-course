{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compressed_sensing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEmm_fdhRP74"
      },
      "source": [
        "# Compressed Sensing\n",
        "\n",
        "<font color='green'> This practical session serves as a gentle introduction to compressed sensing - a signal processing/statistical technique for recovering a sparse signal from an underdetermined system of linear equations. Our main objectives are the following:\n",
        "</font>\n",
        "- <font color='green'>introducing the basis pursuit linear program - a convex relaxation to the combinatorial optimization problem that we want to solve;</font>\n",
        "- <font color='green'>understanding the interplay between the nullspace of the design matrix and the geometry of $\\ell_{1}$ norm that allows for sparse signal recovery via the basis pursuit program;\n",
        "- <font color='green'>introducing the restricted isometry property - a sufficient condition that ensures the success of the basis pursuit program;\n",
        "- <font color='green'>demonstrating (one of many possible) practical applications of the presented theory â€” showing how to utilize compressed sensing ideas to design single-pixel cameras.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkeaKF3oNY4f"
      },
      "source": [
        "The setup of this practical session can be described as follows. We want to recover some signal vector $x^{\\star} \\in \\mathbb{R}^{d}$ given access to $n \\ll d$ linear measurements $y_{i} = \\langle x_{i}, w^{\\star} \\rangle$. In matrix-vector notation, the observations $y_{i}$ follow the model\n",
        "$$\n",
        "  y = X w^{\\star}, \\tag{1}\n",
        "$$\n",
        "where $X \\in \\mathbb{R}^{n \\times d}$.\n",
        "The key difficulty in recovering $w^{\\star}$ given $(x_{i}, y_{i})_{i=1}^{n}$ stems from the fact that the above linear system is *underdetermined*, and so there exist infinitely many $w$ such that $Xw = y$. Indeed, since $d > n$, the nullspace of $X$ is of rank at least $n - d \\geq 1$, and hence it contains an infinite number of vectors. A sum of any such vector with the target vector $w^{\\star}$ yields a candidate solution to the above linear system.\n",
        "\n",
        "However, if the signal vector $w^{\\star}$ is known to be $k$-sparse\n",
        "(with $k \\ll n$), then the above linear system can be solved *efficiently* for certain design matrices $X$. Understanding sufficient conditions for the solvability of underdetermined linear systems as well as some real-world implications are the primary subjects of this practical session.\n",
        "\n",
        "We remark that in this practical session, we work under idealized conditions.\n",
        "In particular, we assume that the linear measurements $y_{i}$ contain no noise and that the target signal vector $w^{\\star}$ is exactly sparse. Various extensions can be obtained for noisy observations as well as approximately sparse target vectors; the interested reader will find pointers to the existing literature at the end of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zdVAd1ABTYJ"
      },
      "source": [
        "## Sparse Recovery via Linear Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OZjotWRNXBh"
      },
      "source": [
        "Let $\\|w\\|_{0}$ denote the \"$\\ell_{0}$ norm\" of $w$, equal to the number of non-zero coordinates of $w$. Given the knowledge that the true signal vector $w^{\\star}$ is sparse, arguably the most natural approach to recovering $w^{\\star}$ from the underdetermined linear system $(1)$ is to look for a vector $w$ with fewest non-zero entries that is consistent with the observations, that is, a solution to the following program:\n",
        "$$\n",
        "  \\min \\|w\\|_{0} \\quad\\text{subject to}\\quad Xw = y\n",
        "$$\n",
        "The combinatorial nature of the above problem, however, presents computational challenges: the naive approach of enumerating all possible subsets of coordinates and trying to solve the above linear system using the selected variables would require exponential running time in\n",
        "the sparsity level $k$ and thus, is infeasible in practice.\n",
        "\n",
        "To circumvent computational issues, we will instead consider\n",
        "replacing the \"$\\ell_{0}$ norm\" with an $\\ell_{q}$ norm for the smallest $q > 0$ that yields a convex program. The smallest such $q$ is given by the choice $q=1$ and thus we will aim to solve the following program, called *basis pursuit*:\n",
        "$$\n",
        "  \\min \\|w\\|_{1}\\quad\\text{subject to}\\quad Xw = y.\n",
        "  \\tag{2}\n",
        "$$\n",
        "\n",
        "<font color='green'>**While the above optimization problem is readily seen to be convex, it can be\n",
        "rephrased as a [linear program](https://en.wikipedia.org/wiki/Linear_programming)**</font>.\n",
        "Such problems, written in a standardized form, can be expressed as\n",
        "\\begin{align*}\n",
        "  &\\min_{x \\in \\mathbb{R}^{d}} \\langle c, x\\rangle \\\\\n",
        "  &\\text{subject to}\\quad\n",
        "  Ax = b\\quad\\text{and}\\quad\n",
        "  Gx \\preccurlyeq h,\n",
        "\\end{align*}\n",
        "where the vectors $c,b,h$ and the matrices $A, G$ are arbitrary problem parameters, and the notation $\\preccurlyeq$ denotes a componentwise inequality. \n",
        "Linear programs can be solved in [weakly polynomial](https://en.wikipedia.org/wiki/Linear_programming#Open_problems_and_recent_work) time by general-purpose solvers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DctUonBGSAhg"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKyBz0OcSCQ0"
      },
      "source": [
        "Show that the convex program $(2)$ can be expressed as a linear program.\n",
        "Use the [cvxopt](https://cvxopt.org/) package (imported in the below cell) to implement a solver for $(2)$ by completing the missing code two cells below. For the cvxopt package documentation concerning linear programs see http://cvxopt.org/userguide/coneprog.html#linear-programming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBoiBuX2UuXg"
      },
      "source": [
        "import numpy as np\n",
        "import cvxopt\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THt6rBpuURyr"
      },
      "source": [
        "def compute_minimum_l1_norm_solution(X, y):\n",
        "  \"\"\" :X: An n \\times d matrix.\n",
        "      :y: An n dimensional array such that y = Xw* for some k-sparse vector w*.\n",
        "      :returns: A vector w that solves the l1 minimization program (2).\n",
        "  \"\"\"\n",
        "  ##############################################################################\n",
        "  # Exercise 1. Fill in the implementation of this function by using\n",
        "  # cvxopt.solvers.lp function.\n",
        "  \n",
        "  ##############################################################################\n",
        "\n",
        "# The below code is designed to test your implementation of the above function.\n",
        "n = 100\n",
        "d = 1000\n",
        "k = 10\n",
        "w_star = np.zeros((d,1))\n",
        "w_star[:k,0] = 1\n",
        "# We will see why the below choice of the measurements matrix X works in\n",
        "# Exercise 2.\n",
        "X = n**(-1/2) * np.random.binomial(n=1, p=0.5, size=(n,d))*2 - 1.0\n",
        "y = X @ w_star\n",
        "w = compute_minimum_l1_norm_solution(X, y)\n",
        "# The below should print approximately 0.\n",
        "print(\"||w - w*||_{2}^{2} =\", np.sum((w.reshape(d, 1) - w_star)**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4k1Q1AdSYAA"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCwwXmLaSZ5m"
      },
      "source": [
        "We can reformulate $(2)$ via the following equivalent linear program, by intruducing an additional variable $t \\in \\mathbb{R}^{d}$:\n",
        "\\begin{align*}\n",
        "  &\\min_{w,t \\in \\mathbb{R}^{d}} \\sum_{i=1}^{d} t_{i} \\\\\n",
        "  &\\text{subject to}\\quad\n",
        "  Xw = y\\quad\\text{and}\\quad\n",
        "  w_{i} \\leq t_{i},\\, w_{i} \\geq -t_{i}\\text{ for }i = 1,\\dots d.\n",
        "\\end{align*}\n",
        "The above formulation can be passed to the cvxopt package as follows:\n",
        "```\n",
        "  n = X.shape[0]\n",
        "  d = X.shape[1]\n",
        "  # Set up the linear programming variables. \n",
        "  c = np.concatenate((np.zeros(d), np.ones(d))).reshape(-1, 1)\n",
        "  A = np.concatenate((X, np.zeros((n, d))), axis=1)\n",
        "  h = np.zeros(2 * d).reshape(-1, 1)\n",
        "  # We will use sparse matrix representation of G.\n",
        "  # Note that cvxopt takes lists columns as arguments rather than lists of rows\n",
        "  # as done by numpy.\n",
        "  Id = cvxopt.spmatrix(1.0, range(d), range(d))\n",
        "  G = cvxopt.sparse([[Id, -Id], [-Id, -Id]])\n",
        "  # Convert c,A,b and h to cvxopt matrices.\n",
        "  c = cvxopt.matrix(c)\n",
        "  A = cvxopt.matrix(A)\n",
        "  b = cvxopt.matrix(y.reshape((n,1)))\n",
        "  h = cvxopt.matrix(h)\n",
        "  # Solve the linear program.\n",
        "  solution = cvxopt.solvers.lp(c, G, h, A, b)\n",
        "  w = np.array(solution['x'][:d]).reshape(-1,1)\n",
        "  return w\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtRDrJHOCZhS"
      },
      "source": [
        "## Restricted Nullspace and Restricted Isometry Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2T8PKxuZlxd"
      },
      "source": [
        "We now investigate the necessary and sufficient conditions under which the basis pursuit program $(2)$ succeeds to recover any $k$-sparse target vector exactly. <font color='green'>**Ultimately, the success of the basis pursuit linear program will depend on the interplay between the nullspace of $X$ and the $\\ell_{1}$ geometry, in the precise sense explained below.**</font> Before proceeding, we introduce some additional notation. Let $S \\subseteq \\{1, \\dots, d\\}$ denote some index set and let $S^{c} = \\{1, \\dots, d\\} \\backslash S$. Given a vector $w \\in \\mathbb{R}^{d}$ we write $w_{S} \\in \\mathbb{R}^{d}$ to denote a restriction of $w$ to the support set $S$ by setting the other coordinates to $0$. Hence, we have $w = w_{S} + w_{S^{c}}$ for any vector $w$ and any support set $S$.\n",
        "\n",
        "\n",
        "Let $\\hat{w}$ denote the output of $(2)$ and let $\\Delta = w^{\\star} - \\hat{w}$.\n",
        "Then, since $\\|\\hat{w}\\|_{1} \\leq \\|w^{\\star}\\|_{1}$ we can deduce that\n",
        "the $\\ell_{1}$ mass of $\\Delta_{S^{c}}$ is at most equal to the $\\ell_{1}$ mass of $\\Delta_{S}$:\n",
        "\\begin{align*}\n",
        "  \\|\\Delta_{S^{c}}\\|_{1} \n",
        "  &= \\|\\hat{w}_{S^{c}}\\|_{1} \\\\\n",
        "  &\\leq \\|\\hat{w}_{S^{c}}\\|_{1} + \\underbrace{(\\|w^{\\star}\\|_{1} - \\|\\hat{w}\\|_{1})}_{\\geq 0 \\text{ by definition of } (2)} \\\\\n",
        "  &= \\|w^{\\star}\\|_{1} - \\|\\hat{w}_{S}\\| \\\\\n",
        "  &= \\|w^{\\star}_{S}\\|_{1} - \\|\\hat{w}_{S}\\| & \\text{since }w^{\\star}\\text{ is supported on }S \\\\\n",
        "  &\\leq \\|w^{\\star}_{S} - \\hat{w}_{S}\\|_{1} \\\\\n",
        "  &= \\|\\Delta_{S}\\|_{1}.\n",
        "\\end{align*}\n",
        "In particular, the residual vector $\\Delta$ belongs to the cone $\\mathcal{C}(S)$ defined as\n",
        "$$\n",
        "  \\Delta \\in \\mathcal{C}(S) = \\{\\Delta \\in \\mathbb{R}^{d} : \\|\\Delta_{S^{c}}\\|_{1} \\leq \\|\\Delta_{S}\\|_{1}\\}.\n",
        "$$\n",
        "Notice that since $X\\hat{w} = y = Xw^{\\star}$, we also have $X \\Delta = 0$, \n",
        "that is, $\\Delta \\in \\mathrm{ker} X = \\{w \\in \\mathbb{R}^{d} : Xw = 0\\}$. It \n",
        "follows that $\\mathrm{ker} X \\cap \\mathcal{C}(S) = \\{0\\}$ is a *sufficient* \n",
        "condition to ensure that the basis pursuit program $(2)$ succeeds to output a vector $\\hat{w} = w^{\\star}$.\n",
        "\n",
        "In fact, the above condition is also *necessary*. To see that, suppose for a contradiction that\n",
        "$(2)$ succeeds to recover the correct solution for any underlying vector\n",
        "$w^{\\star}$ supported on $S$ despite the existence of some non-zero $y \\in \\mathrm{ker} K \\cap \\mathcal{C}(S)$.\n",
        "Set $w^{\\star} = y_{S}$ and note that $Xw^{\\star} = X(-y_{S^{c}})$. At the same time, we have $\\|y_{S^{c}}\\|_{1} \\leq \\|w^{\\star}\\|_{1}$, and thus the basis pursuit program $(2)$ can output $y_{S^{c}} \\neq w^{\\star}$, which contradicts the assumption that $(2)$ always succeeds to output the correct vector $w^{\\star}$.\n",
        "\n",
        "\n",
        "We can thus formulate a necessary and sufficient condition for the success of basis pursuit program for any target vector $w^{\\star}$ supported on $S$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPDuPaz3CMhL"
      },
      "source": [
        "---\n",
        "\n",
        "**Restricted Nullspace Property (RNP)**\n",
        "\n",
        "A matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfies the restricted nullspace property with respect to an index set $S \\subseteq \\{1, \\dots, d\\}$ if\n",
        "$\\mathrm{ker} X \\cap \\mathcal{C}(S) = \\{0\\}$, where\n",
        "$\n",
        " \\mathcal{C}(S) = \\{ \\Delta \\in \\mathbb{R}^{d} : \\|\\Delta_{S^{c}}\\|_{1} \\leq \\|\\Delta_{S}\\|_{1}\\}.\n",
        "$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIIXZR1vCitc"
      },
      "source": [
        "Since we want the basis pursuit problem to succeed for any $k$-sparse vector $w^{\\star}$, we want the measurement matrix $X$ to satisfy the RNP uniformly over all support sets $S$ of size $k$. There is, unfortunately, no easy way to verify wheter a given matrix $X$ satisfies uniform RNP; <font color='green'>**however, there exist sufficient conditions that imply the uniform RNP.  On such condition, called *the restricted isometry property*, is stated below.**</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Restricted Isometry Property (RIP)**\n",
        "\n",
        "A matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfies a $(k, \\delta)$-restricted isometry property if the following deterministic inequality holds for any k-sparse vector $w \\in \\mathbb{R}^{d}$:\n",
        "$$\n",
        "  (1-\\delta)\\|w\\|_{2}^{2}\n",
        "  \\leq \\|Xw\\|_{2}^{2}\n",
        "  \\leq\n",
        "  (1 + \\delta)\\|w\\|_{2}^{2}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Let $X_{S} \\in \\mathbb{R}^{n \\times |S|}$ denote a subset of $X$ formed by only including the columns indexed by some support set $S \\subseteq \\{1, \\dots, n\\}$.\n",
        "Suppose that $w$ is a k-sparse vector with support $S$, that is, $w = w_{S}$.\n",
        "Let $\\tilde{w}_{S} \\in \\mathbb{R}^{|S|}$ denote the projection of $w_{S}$ onto the coordinates indexed by $S$ (dropping the other coordinates). Then, we have\n",
        "$$\n",
        "  \\|Xw_{S}\\|_{2}^{2} = \\|X_{S}\\tilde{w}_{S}\\|_{2}^{2} = \\tilde{w}_{S}^{\\mathsf{T}}X_{S}^{\\mathsf{T}}X_{S}\\tilde{w}_{S}.\n",
        "$$\n",
        "It follows that the $(k, \\delta)$-RIP can be restated as:\n",
        "$$\n",
        "  \\left\\|\n",
        "    X_{S}^{\\mathsf{T}}X_{S} - I\n",
        "  \\right\\|_{\\mathrm{op}} \\leq \\delta\n",
        "  \\text{ for any }S \\subseteq\\{1, \\dots, d\\}\\text{ such that }|S| \\leq k,\n",
        "$$\n",
        "where $\\|A\\|_{\\mathrm{op}} = \\sup_{\\|u\\|_{2} \\leq 1} \\|Au\\|_{2}$ denotes the\n",
        "$\\ell_{2} \\to \\ell_{2}$ operator norm.\n",
        "<font color='green'>**In words, the RIP property states that for any $S \\subseteq \\{1, \\dots, d\\}$ of size $k$, the matrix $X_{S}^{\\mathsf{T}}X_{S}$ is approximately equal to the identity matrix.**</font> We will now prove that the RIP indeed implies the RNP.\n",
        "\n",
        "---\n",
        "\n",
        "**Theorem**\n",
        "\n",
        "Suppose that a matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfies $(2k, \\delta)$-RIP with $\\delta \\in (0, \\frac{1}{3})$.\n",
        "Then, $X$ satisfies the RNP for any index set $S$ of size at most $k$.\n",
        "\n",
        "\n",
        "> **Proof**\n",
        ">\n",
        ">  - We will write $\\delta$ for the $(2k,\\delta)$-RIP constant of $X$ and deduce in the end that $\\delta = 1/3$ suffices.\n",
        ">  - Fix any $\\Delta \\in \\mathrm{ker} X$ such that $\\Delta \\neq 0$. Let $S_{0}$ denote a subset of $\\{1, \\dots, d\\}$ of size $k$ where $\\Delta$ has largest coordinates in absolute value. To prove the above theorem it suffices to show that $\\|\\Delta_{S_{0}^{c}}\\|_{1} > \\|\\Delta_{S_{0}}\\|_{1}$.\n",
        ">  - Decompose $\\Delta_{S_{0}^{c}}$ into $\\sum_{i \\geq 1} \\Delta_{S_{i}}$, where $S_{1}$ indicates a subset of $S_{0}^{c}$ with largest absolute values of $\\Delta_{S^{c}}$, $S_{2}$ indicates a subset of $S_{0}^{c} \\backslash S_{1}$ with largest absolute values of $\\Delta$, etc. All subsets $S_{i}$, are of size $|S| = k$, possibly except for the last subset which may contain fewer indices.\n",
        ">  - By definition of the subsets $S_{0}, S_{1}, S_{2}, \\dots$, it follows that \n",
        "for any $i \\geq 1$ we have $\\|\\Delta_{S_{i}}\\|_{2} \\leq \\sqrt{s}\\|\\Delta_{S_{i}}\\|_{\\infty} \\leq s^{-1/2}\\|\\Delta_{S_{i-1}}\\|_{1}$. It follows that \n",
        "$$\n",
        "  \\sum_{i \\geq 1} \\|\\Delta_{S_{j}}\\|_{2}\n",
        "  \\leq s^{-1/2}(\\|\\Delta_{S_{0}}\\|_{1} + \\|\\Delta_{S_{0}^{c}}\\|_{1}).\n",
        "$$\n",
        ">  - Since $\\Delta \\in \\mathrm{ker} X$, we have\n",
        "$X\\Delta_{S_{0}} = -X\\Delta_{S_{0}^{c}} = -X\\sum_{i \\geq 1}\\Delta_{S_{i}}$.\n",
        "Applying the RIP assumption, it follows that\n",
        "$$\n",
        "  (1-\\delta)\\|\\Delta_{S_{0}}\\|_{2}^{2} \\leq \\|X\\Delta_{S_{0}}\\|_{2}^{2}\n",
        "  = \\langle X \\Delta_{S_{0}}, -X\\sum_{i \\geq 1}\\Delta_{S_{i}}\\rangle\n",
        "  \\leq \\left|\n",
        "  \\sum_{i \\geq 1}\\langle X \\Delta_{S_{0}}, X \\Delta_{S_{i}}\\rangle\n",
        "  \\right|\n",
        "$$\n",
        "Now, noting that for any $i \\geq 1$, $S_{0}$ and $S_{i}$ are disjoint. Hence, $\\langle \\Delta_{S_{0}}, \\Delta_{S_{i}} \\rangle = 0$ and we can write \n",
        " $$\n",
        "   | \\langle X \\Delta_{S_{0}}, X \\Delta_{S_{i}} \\rangle |\n",
        "   =\n",
        "   | \\langle X \\Delta_{S_{0}}, X \\Delta_{S_{i}} \\rangle \n",
        "   - \\langle \\Delta_{S_{0}}, \\Delta_{S_{i}} \\rangle |\n",
        "   =\n",
        "   | \\langle \\Delta_{S_{0}},\n",
        "      (X^{\\mathsf{T}}_{S_{0} \\cup S_{i}}X_{S_{0} \\cup S_{i}} - I) \\Delta_{S_{i}} \\rangle \n",
        "   \\leq \\delta \\|\\Delta_{S_{0}}\\|_{2}\\|\\Delta_{S_{i}}\\|_{2}.\n",
        " $$\n",
        " Combining the above two inequalities, it follows that\n",
        " $$\n",
        "    \\|\\Delta_{S_{0}}\\|_{2} \\leq\n",
        "    \\frac{\\delta}{1-\\delta}\\sum_{i\\geq 1}\\|\\Delta_{S_{i}}\\|_{2}.\n",
        " $$\n",
        ">  - Putting everything together yields\n",
        "$$\n",
        "  \\|\\Delta_{S_{0}}\\|_{1}\n",
        "  \\leq \\sqrt{s}\\|\\Delta_{S_{0}}\\|_{2}\n",
        "  \\leq \n",
        "  \\sqrt{s}\\frac{\\delta}{1-\\delta}\\sum_{i\\geq 1}\\|\\Delta_{S_{i}}\\|_{2}\n",
        "  \\leq \\frac{\\delta}{1- \\delta}(\\|\\Delta_{S_{0}}\\|_{1} + \\|\\Delta_{S_{0}^{c}}\\|_{1}).\n",
        "$$\n",
        "Rearranging, we obtain\n",
        "$$\n",
        "  \\|\\Delta_{S_{0}}\\|_{1} \\leq \\frac{\\delta}{1 - 2\\delta}\\|\\Delta_{S_{0}^{c}}\\|_{1}.\n",
        "$$\n",
        "Since for any $\\delta \\in (0,1/3)$ we have $0 < \\frac{\\delta}{1-2\\delta} < 1$,\n",
        "our proof is complete. \n",
        "\n",
        "---\n",
        "\n",
        "In the next exercise, we show that random matrices with i.i.d. sub-Gaussian entries satisfy RIP with high probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0qf8vbsu0Ip"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btm-BLoMqbwN"
      },
      "source": [
        "Let $X$ denote an $n \\times m$ matrix such that $n \\geq m$ and the $i,j$-th entry $X_{i,j}$ is sampled i.i.d. from a zero-mean $1$-subGaussian distribution (recall that $Y \\sim P$ is $1$-subGaussian if $\\mathbf{E}[e^{\\lambda Y}] \\leq e^{\\lambda^{2}/2}$). Using results from random matrix theory, it can be shown that\n",
        "for any $\\varepsilon \\in (0,1)$ we have\n",
        "$$\n",
        "  \\mathbb{P}\\left( \\left\\|\\frac{1}{n}X^{\\mathsf{T}}X - \\mathbf{E}\\left[\\frac{1}{n}X^{\\mathsf{T}}X\\right]\\right\\|_{\\mathrm{op}} \\geq c_{1}\\sqrt{\\frac{m}{n}} + \\varepsilon \\right) \\leq \\exp(-c_{3} n \\varepsilon^{2}),\n",
        "$$\n",
        "where $c_{1}, c_{2}$ and $c_{3}$ are absolute constants.\n",
        "For example, see Theorem 6.5 in the textbook by *Wainwright [2019]* for a more general statement and the proof of the above claim.\n",
        "\n",
        "Suggest a way to obtain an $n \\times d$ matrix (with $n \\ll d$) that satisfies $(2k, 1/3)$-RIP with high probability. Prove that it suffices to take $n = c'k\\log(d/k)$ for some absolute constant $c'$. <font color='green'>**In particular, n needs to grow only linearly with respect to the sparsity parameter $k$ and logarithmically with the dimension $d$.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSqSQBgk_NFG"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-zMRTGt_Ohj"
      },
      "source": [
        "We sample i.i.d. entries $X_{i,j}$ from some zero-mean $1$-sub-Gaussian distribution with variance equal to $1$ (among other examples, such conditions are satisfied by i.i.d. standard normal or Rademacher random variables). We will apply the given concentration result for all sub-matrices $X_{S}$ with $S \\subseteq \\{1, \\dots, d\\}$ and $|S| \\leq 2k$ and conclude via the union bound that $\\frac{1}{\\sqrt{n}}X$ satisfies $(2k, \\delta)-RIP$ with high probability.\n",
        "\n",
        "Note that for any $S$ we have $\\mathbf{E}[X_{S}^{\\mathsf{T}}X_{S}/n] = I$. Thus, for any fixed $S$ we have\n",
        "$$\n",
        "  \\mathbb{P}\\left( \\left\\|\\frac{1}{n}X_{S}^{\\mathsf{T}}X_{S} - I\\right\\|_{\\mathrm{op}} \\geq c_{1}\\sqrt{\\frac{2k}{n}} + \\varepsilon \\right) \\leq \\exp(-c_{3} n \\varepsilon^{2}),\n",
        "$$\n",
        "For $n \\geq 72c_{1}^{2}k$ we have $c_{1}\\sqrt{\\frac{2k}{n}} \\leq \\frac{1}{6}$.\n",
        "Setting $\\varepsilon = \\frac{1}{6}$, $c_{4} = 72c_{1}^{2}$ and $c_{5} = c_{3}/36$ we have\n",
        "$$\n",
        "  \\mathbb{P}\\left( \\left\\|\\frac{1}{n}X_{S}^{\\mathsf{T}}X_{S} - I\\right\\|_{\\mathrm{op}} \\geq \\frac{1}{3} \\right) \\leq \\exp(-c_{5} n),\n",
        "$$\n",
        "Note that $(2k, \\delta)$-RIP fails to hold for $\\frac{1}{\\sqrt{n}}X$ if and only if there exists some subset $S$ with $|S| \\leq 2k$ such that the above event of probability $\\exp(-c_{5}n)$ happens. Since the number of possible subsets $S$ can be upper bounded as $\\sum_{i=1}^{2k} \\binom{d}{i} \\leq (\\frac{ed}{2k})^{2k}$, by the union bound we have\n",
        "$$\n",
        "  \\mathbb{P}\\left(\\frac{1}{\\sqrt{n}}X \\text{ fails to satisfy $(2k, 1/3)$-RIP}\\right)\n",
        "  \\leq \\left(\\frac{ed}{2k}\\right)^{2k}\\exp(-c_{5}n).\n",
        "$$\n",
        "Thus, setting $n = \\frac{1}{c_5}\\log\\left(\\frac{ed}{2k}\\right)^{2k} + \\frac{1}{c5}\\log\\left(\\frac{1}{\\delta}\\right) \\sim k \\log(d/k) + \\log(1/\\delta)$ suffices to ensure that $\\frac{1}{\\sqrt{n}}X$ satisfies $(2k,\\delta)$-RIP with probability at least $1-\\delta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3rw-UcNBTiU"
      },
      "source": [
        "## Single-Pixel Camera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n73R7BLbzl4e"
      },
      "source": [
        "Many real-world signals are structured and highly compressible, meaning they have approximately sparse representations in some appropriately chosen basis.\n",
        "As a typical example, suppose that our signal $w^{\\star} \\in \\mathbb{R}^{d}$ represents an image, where different values of $w^{\\star}_{i}$ represent the colour intensities of the $i$-th pixel. While real-world images represented by pixel intensities are not sparse in the standard basis, they contain redundant patterns of information that can be efficiently compressed via an appropriate change of basis $\\alpha^{\\star} = \\Phi w^{\\star}$, where $\\Phi \\in \\mathbb{R}^{d \\times d}$ is an orthonormal change-of-basis matrix, and $\\alpha^{\\star}$ is an approximately sparse vector.\n",
        "\n",
        "Suppose that we take linear measurements $y_{i} = \\langle x_{i}, w^{\\star}\\rangle$ of the signal $w^{\\star}$. Since $w^{\\star}$ is not sparse in the standard basis, solving\n",
        "$$\n",
        "  \\min \\|w\\|_{1} \\text{ subject to } Xw = y\n",
        "$$\n",
        "will not yield a desirable solution. Instead, note that we can reformulate the above in the transformed coordinate system $\\alpha = \\Phi w$ as\n",
        "$$\n",
        "  \\min \\|\\alpha\\|_{1}\n",
        "  \\text{ subject to } X\\Phi\\alpha = y = X\\Phi\\alpha^{\\star}.\n",
        "$$\n",
        "If $\\alpha^{\\star}$ is indeed sparse and if $X\\Phi$ satisfies the restricted nullspace property, then we are guaranteed to recover the correct solution $\\alpha^{\\star}$ via the basis pursuit linear program. The image can then be transformed to the standard basis by taking $w^{\\star} = \\Phi^{\\mathsf{T}}\\alpha^{\\star}$. Although we have not proved this in exercise 2, given an orthonormal matrix $\\Phi$ and a random ensemble $X$ consisting of i.i.d. zero-mean $1$-sub-Gaussian random variables (e.g., Rademacher random variables), the matrix $\\frac{1}{\\sqrt{n}}X\\Phi$ indeed satisfies RIP with high probability. See the discussion section in [a paper by Baraniuk, Davenport, DeVore and Wakin](https://users.math.msu.edu/users/iwenmark/Teaching/MTH995/Papers/JL_RIP_Proof.pdf) for further details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bDJLMNcA5QN"
      },
      "source": [
        "What we have described above forms the conceptual basis for an emerging technology of single-pixel cameras, [introduced by a team of researchers from Rice University](https://scholarship.rice.edu/bitstream/handle/1911/21682/csCamera-SPMag-web.pdf;jsessionid=AA8AD0D7D0FB3FB37624B270D3495D8B?sequence=1), which we are going to walk through in the remainder of this section. <font color='green'>**The idea of single-pixel cameras is to combine sampling and compression into a single step by directly trying to sense the sparsified signal $\\alpha^{\\star}$, without first trying to recover the signal $w^{\\star}$ in the standard basis.**</font> This should be contrasted with standard camera architectures, where the signal is first acquired in the standard basis, and only later it is compressed for storage or transmission purposes.\n",
        "\n",
        "Informally, the hardware implementation of a single-pixel camera consists of two lenses, an array of $d$ micro-mirrors, a single photon detector and an analog-to-digital signal converter. The incoming light is first oriented via the first lens onto an array of micro-mirrors. Each mirror can reflect light in one of two possible directions, depending on the orientation of the mirror, which changes between different measurements (thus implementing the Rademacher ensemble for the measurements matrix $X$). The reflected light from the mirrors is then focused by the second lens onto a photon detector that computes the measurement $y_{i} = \\langle w^{\\star}, x_{i}\\rangle$. The measurements are then passed to a digital computer via an analog-to-digital converter component of the camera. Among the benefits offered by single-pixel cameras are reduced costs due to the single photon detector design (especially concerning applications going beyond the scope of consumer photography)\n",
        "or reduced sampling time, in comparison to classical multiplexed architectures that try to acquire the signal in the standard basis and hence need to take $d \\gg c k \\log(d/k)$ measurements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSMpiJ9MKP9g"
      },
      "source": [
        "We now turn to illustrating the above-outlined ideas via simulations. First, we will load an image that we will use in our simulations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_h9zAMqz0nm"
      },
      "source": [
        "from skimage import data\n",
        "\n",
        "cameraman = np.array(data.camera(), dtype=np.float32)\n",
        "cameraman -= 128 # Normalize the pixel values.\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(cameraman, cmap='gray', interpolation='nearest')\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs-rWCInKsed"
      },
      "source": [
        "To perform the change of basis $\\alpha = \\Phi w$, we will use a [two-dimensional discrete cosine transform](https://en.wikipedia.org/wiki/Discrete_cosine_transform#M-D_DCT-II) on non-overlapping $8 \\times 8$ blocks of the target image. An implementation of a one-dimensional discrete cosine transform is provided by [scipy.fft](https://docs.scipy.org/doc/scipy/reference/tutorial/fft.html) package. In the below code, we implement two-dimensional discrete cosine transforms and display the $64$ basis vectors as $8\\times 8$ images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO8BLApowDYs"
      },
      "source": [
        "from scipy.fft import dct, idct\n",
        "\n",
        "def dct2(block):\n",
        "  return dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
        "\n",
        "def idct2(block):\n",
        "  return idct(idct(block, axis=1, norm='ortho'), axis=0, norm='ortho')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijh5e8qc3Xow"
      },
      "source": [
        "I = np.identity(64)\n",
        "fig, ax = plt.subplots(8, 8)\n",
        "fig.set_size_inches(8,8)\n",
        "Phi = np.zeros((64, 64)) # A linear map implemented by dct2.\n",
        "for i in range(8):\n",
        "  for j in range(8):\n",
        "    block = I[:,8*i+j].reshape(8, 8)\n",
        "    Phi[:,8*i+j] = idct2(block).reshape(-1)\n",
        "    ax[i,j].imshow(Phi[:, 8*i+j].reshape(8,8), cmap='gray')\n",
        "    ax[i,j].axis('off')\n",
        "\n",
        "print(\"Is Phi orthonormal:\", np.allclose(Phi.T @ Phi, np.identity(64)) \\\n",
        "                             and np.allclose(Phi @ Phi.T, np.identity(64)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXrZ_gq1M-YU"
      },
      "source": [
        "Notice that the displayed basis vectors increase horizontal frequencies as we move to the right in the horizontal direction. Likewise, it increases the vertical frequencies as we move in the vertical direction downwards. Thus, the upper-left basis vectors represent low-frequency parts of the image, while the lower-right basis vectors would be used to represent high-frequency components of the image. <font color='green'>**As the real-world images are structured, we expect that most parts of the image can be expressed by primarily relying on the low-frequency basis vectors, thus resulting in sparse representations.**</font> In contrast, note that we would not expect an unstructured image comprised of completely random pixels to admit a sparse representation in the above basis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHb672QXOxyK"
      },
      "source": [
        "We now implement code that slides along non-overlapping $8 \\times 8$ blocks of a target image and applies a discrete cosine transform to each of the blocks. We observe that the transformed cameraman image admits an approximately sparse representation in the transformed coordinate system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caHTCzJlOI-q"
      },
      "source": [
        "def transform_image(image, transformation):\n",
        "  # For simplicity assume that the image dimensions are divisible by 8.\n",
        "  assert image.shape[0] % 8 == 0\n",
        "  assert image.shape[1] % 8 == 0\n",
        "  assert len(image.shape) == 2\n",
        "\n",
        "  transformed_image = np.zeros_like(image, dtype=np.float32)\n",
        "  for i in range(image.shape[0]//8):\n",
        "    for j in range(image.shape[1]//8):\n",
        "      block = image[8*i:8*(i+1), 8*j:8*(j+1)]   \n",
        "      transformed_block = transformation(block)\n",
        "      transformed_image[8*i:8*(i+1), 8*j:8*(j+1)] = transformed_block\n",
        "  return transformed_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovm6cevJhKFl"
      },
      "source": [
        "print(\"Original image:\")\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cameraman, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(\"Absolue values of coefficients of transformed image:\")\n",
        "plt.figure(figsize=(6,6))\n",
        "encoded_cameraman = transform_image(cameraman, dct2)\n",
        "sorted_coefficients = np.sort(np.abs(encoded_cameraman.flatten()))\n",
        "# Set maximum size to display as white color.\n",
        "vmax = sorted_coefficients[-int(len(sorted_coefficients)*0.075)]\n",
        "plt.imshow(np.abs(encoded_cameraman), cmap='gray', vmax = vmax, vmin = 0)\n",
        "plt.show()\n",
        "\n",
        "print(\"Reconstructed image:\")\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(transform_image(encoded_cameraman, idct2), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohKu5M8eTYhu"
      },
      "source": [
        "A typical compression scheme would *quantize* the `encoded_cameraman` variable by setting small coefficients to $0$ (there are smarter ways to do it by also taking into account whether the given coefficient represents a high-frequency or low-frequency basis vector; we may want to be more inclined to quantize high-frequency components. See the wikipedia page on [JPEG compression](https://en.wikipedia.org/wiki/JPEG#JPEG_compression)). Let us now try a simple quantizing scheme that simply keeps a given fraction `p` of coefficients largest in absolute value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erSMRGxFUEhl"
      },
      "source": [
        "def quantize_coefficients(transformed_image, p):\n",
        "  \"\"\" :transformed_image: An image represented in the transformed basis.\n",
        "      :p: A fraction of largest coefficients to keep.\n",
        "      :returns: A quantized image, retaining p-fraction of the largest\n",
        "        coefficients in absolute value.\n",
        "  \"\"\"\n",
        "  sorted = np.sort(np.abs(transformed_image).flatten())\n",
        "  threshold = sorted[-int(len(sorted)*p)]\n",
        "  quantized_image = np.copy(transformed_image)\n",
        "  quantized_image[np.abs(quantized_image) < threshold] = 0\n",
        "  return quantized_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYGli5aSVCHB"
      },
      "source": [
        "print(\"Original image:\")\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cameraman, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(\"Absolue values of coefficients of transformed image:\")\n",
        "plt.figure(figsize=(6,6))\n",
        "encoded_cameraman = transform_image(cameraman, dct2)\n",
        "sorted_coefficients = np.sort(np.abs(encoded_cameraman.flatten()))\n",
        "# Set maximum size to display as white color.\n",
        "vmax = sorted_coefficients[-int(len(sorted_coefficients)*0.075)]\n",
        "plt.imshow(np.abs(encoded_cameraman), cmap='gray', vmax = vmax, vmin = 0)\n",
        "plt.show()\n",
        "\n",
        "p = 0.025 # The fraction of coefficients to keep.\n",
        "print(\"Quantized image at level p=\",p)\n",
        "quantized_image = quantize_coefficients(encoded_cameraman, p)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(np.abs(quantized_image), cmap='gray', vmax=vmax, vmin=0)\n",
        "plt.show()\n",
        "\n",
        "print(\"Reconstruction from the quantized image:\")\n",
        "plt.figure(figsize=(6,6))\n",
        "reconstructed_image = transform_image(quantized_image, idct2)\n",
        "plt.imshow(reconstructed_image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(\"Average l_2 squared reconstruction error:\",\n",
        "      np.average((cameraman.flatten() - reconstructed_image.flatten())**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py8glXkUW8sD"
      },
      "source": [
        "In the next exercise, you are asked to implement a single-pixel camera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcPxogAdXDLJ"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzz7dCVBXE6r"
      },
      "source": [
        "- Complete the single-pixel camera implementation in the below cell.\n",
        "- How many measurements need to be taken to recover a \"visually acceptable\" `cameraman` image? How does the number of measurements compare with the number of pixels in the image?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxkkkr-fXJKj"
      },
      "source": [
        "class SinglePixelCamera(object):\n",
        "  \"\"\" An implementation of a single pixed camera. \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, d1, d2):\n",
        "    \"\"\" :(d1, d2): Shape of the image. \"\"\"\n",
        "    self.d1 = d1\n",
        "    self.d2 = d2\n",
        "\n",
        "\n",
        "  def get_mirror_positions(self, n_measurements):\n",
        "    \"\"\" Returns a numpy array of shape (n_measuremes, self.d1 * self.d2), whose\n",
        "    i-th row stores {-1, +1}-valued mirror positions to be used for the\n",
        "    i-th linear measurement.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Complete the below implementation.\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "  def take_measurements(self, X, w_star):\n",
        "    \"\"\" :X: A measurement matrix of shape (n, d1*d2).\n",
        "        :w_star: The signal vector (represented as a 2d-array) that we are\n",
        "          trying to measure.\n",
        "        :returns: A vector of linear measurements of (an appropriately\n",
        "          flattened) image w_star.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Complete the below implementation.\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "  def reconstruct_signal(self, X, y):\n",
        "    \"\"\" :X: An n \\times d measurements matrix.\n",
        "        :y: An n \\times 1 vector of observations.\n",
        "        :returns: A vector w that should approximately recover the true signal\n",
        "          w_star.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Complete the below implementation.\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "  def take_picture(self, n_measurements, w_star):\n",
        "    \"\"\" :n_measurements: The number of measurements y_i = <x_i, w*> to take.\n",
        "        :w_star: The signal vector passed as a two-dimensional image.\n",
        "        :returns: A picture represented in the standard basis, returned as a\n",
        "          two-dimensional array.\n",
        "    \"\"\"\n",
        "    X = self.get_mirror_positions(n_measurements)\n",
        "    y = self.take_measurements(X, w_star)\n",
        "    return self.reconstruct_signal(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWKbiDh-N46V"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Let us test our implementation.\n",
        "# The cameraman image that we have used below is too big for the purpose of \n",
        "# testing out the implementation of single-pixel camera.\n",
        "# Instead, we will switch to a data.microaneurysms() image (which is approximately\n",
        "# 25 times smaller in size).\n",
        "#\n",
        "# WARNING: this can take few couple minutes to execute even for this relatively\n",
        "# small problem. The reason for slow reconstruction is because we are using a\n",
        "# general-purpose solver for our l1 minimization problem. Specialized solvers\n",
        "# can achieve much faster performance.\n",
        "\n",
        "test_image = np.array(data.microaneurysms(), dtype=np.float32)\n",
        "test_image = test_image[:96, :96] # Make dimensions divisible by 8.\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "\n",
        "height = test_image.shape[0]\n",
        "width = test_image.shape[1]\n",
        "camera = SinglePixelCamera(height, width)\n",
        "print(\"Original image:\")\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(\"Number of pixels:\", height*width)\n",
        "n_measurements = (height*width)//4\n",
        "print(\"Number of measurements:\", n_measurements)\n",
        "print(\"Reconstructed image:\")\n",
        "picture = camera.take_picture(n_measurements, test_image)\n",
        "plt.imshow(picture, cmap='gray')\n",
        "print(\"Average l2 reconstruction error:\",\n",
        "      np.average((test_image - picture)**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrGNybLwQ6om"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opa5gjzmQ8nb"
      },
      "source": [
        "A possible implementation is provided below.\n",
        "\n",
        "```\n",
        "class SinglePixelCamera(object):\n",
        "  \"\"\" An implementation of a single pixed camera. \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, d1, d2):\n",
        "    \"\"\" :(d1, d2): Shape of the image. \"\"\"\n",
        "    self.d1 = d1\n",
        "    self.d2 = d2\n",
        "\n",
        "\n",
        "  def get_mirror_positions(self, n_measurements):\n",
        "    \"\"\" Returns a numpy array of shape (n_measuremes, self.d1 * self.d2), whose\n",
        "    i-th row stores {-1, +1}-valued mirror positions to be used for the\n",
        "    i-th linear measurement.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Complete the below implementation.\n",
        "    # Sample mirror positions as i.i.d. Rademacher random variables.\n",
        "    return np.random.binomial(n=1, p=0.5,\n",
        "                              size=(n_measurements, self.d1*self.d2))*2.0 - 1.0\n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "  def take_measurements(self, X, w_star):\n",
        "    \"\"\" :X: A measurement matrix of shape (n, d1*d2).\n",
        "        :w_star: The signal vector (represented as a 2d-array) that we are\n",
        "          trying to measure.\n",
        "        :returns: A vector of linear measurements of (an appropriately\n",
        "          flattened) image w_star.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Complete the below implementation.\n",
        "\n",
        "    # First, we flatten the image array w_star by flattening each 8 x 8 block.\n",
        "    print(\"Started taking measurements\")\n",
        "    flattened_image = np.zeros(self.d1 * self.d2)\n",
        "    offset = 0\n",
        "    for i in range(self.d1//8):\n",
        "      for j in range(self.d2//8):\n",
        "        block = w_star[8*i:8*(i+1), 8*j:8*(j+1)]   \n",
        "        flattened_image[offset:offset+64] = block.flatten()\n",
        "        offset += 64\n",
        "\n",
        "    # We can now compute the linear measurements.\n",
        "    flattened_image.reshape(-1,1) \n",
        "    y = X @ flattened_image\n",
        "    return y\n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "  def reconstruct_signal(self, X, y):\n",
        "    \"\"\" :X: An n \\times d measurements matrix.\n",
        "        :y: An n \\times 1 vector of observations.\n",
        "        :returns: A vector w that should approximately recover the true signal\n",
        "          w_star.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Complete the below implementation.\n",
        "\n",
        "    # Transform X to X @ Phi (note that Phi=block-diagonal[Phi, Phi, ..., Phi]).\n",
        "    blocks = []\n",
        "    for block_id in range(X.shape[1]//64):\n",
        "      blocks.append(X[:,block_id*64:(block_id+1)*64] @ Phi)\n",
        "    transformed_X = np.concatenate(blocks, axis=1)\n",
        "    transformed_X /= np.sqrt(transformed_X.shape[0]) # For numeric stability.\n",
        "    y /= np.sqrt(transformed_X.shape[0]) # Now we also need to rescale y.\n",
        "\n",
        "    # Apply the basis pursuit solver in the transformed coordinate system.\n",
        "    print(\"Starting signal reconstruction.\")\n",
        "    alpha = compute_minimum_l1_norm_solution(transformed_X, y)\n",
        "\n",
        "    # Now we need to transform alpha into the original coordinate system\n",
        "    # and reshape the image into an array of shape (self.d1, self.d2).\n",
        "    reconstructed_image = np.zeros((self.d1, self.d2))\n",
        "    offset = 0\n",
        "    for i in range(self.d1//8):\n",
        "      for j in range(self.d2//8):\n",
        "        alpha_block = alpha[offset:offset+64]\n",
        "        transformed_block = (Phi @ alpha_block).reshape(8, 8)\n",
        "        reconstructed_image[8*i:8*(i+1), 8*j:8*(j+1)] = transformed_block\n",
        "        offset += 64\n",
        "\n",
        "    return reconstructed_image\n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "  def take_picture(self, n_measurements, w_star):\n",
        "    \"\"\" :n_measurements: The number of measurements y_i = <x_i, w*> to take.\n",
        "        :w_star: The signal vector passed as a two-dimensional image.\n",
        "        :returns: A picture represented in the standard basis, returned as a\n",
        "          two-dimensional array.\n",
        "    \"\"\"\n",
        "    X = self.get_mirror_positions(n_measurements)\n",
        "    y = self.take_measurements(X, w_star)\n",
        "    return self.reconstruct_signal(X, y)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxofckZlBPHN"
      },
      "source": [
        "## Bibliographic Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jrFdYKPtF7"
      },
      "source": [
        "The basis pursuit linear program is attributed to the seminal work of *Chen, Donoho, and Saunders [1998]*, with the closely related lasso program introduced in the statistics literature by *Tibshirani [1996]*. The theoretical foundations of compressed sensing, provably demonstrating that sparse signals can be recovered from underdetermined linear systems, were laid down in the pioneering works of *Candes and Tao [2005]*, *Donoho [2006]* and\n",
        "*CandÃ¨s, Romberg, and Tao [2006]*. The proof that RIP implies RNP presented in this practical session was taken from the paper by *Candes [2008]* and Proposition 7.11 in the textbook by *Wainwright [2019]*. The idea of single-pixel cameras was introduced by *Duarte, Davenport, Takhar, Laska, Sun, Kelly, and Baraniuk [2008]*; for the current state of single-\n",
        "pixel imaging, see the recent review paper by *Gibson, Johnson, and Padgett [2020]*. One (perhaps surprising) extension of the results presented in this practical session is that access to linear measurements is not necessary to recover the underlying sparse signal. Indeed, linear programming can be used to recover sparse signals from underdetermined systems of signs of linear measurements, as shown in the work of *Plan and Vershynin [2013]*. In this\n",
        "practical session, we have only glimpsed into the rich theory of compressed sensing and $\\ell_{1}$ regularization. The interested reader will find more advanced topics, further discussions and extensive bibliographic details in the textbooks by *BÃ¼hlmann and Van De Geer [2011]*,\n",
        "*Foucart and Rauhut [2013]*, *Hastie, Tibshirani, and Wainwright [2019]* and *Wainwright [2019]*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ26voWWPtXN"
      },
      "source": [
        "**References**\n",
        "\n",
        "P. BÃ¼hlmann and S. Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science & Business Media, 2011.\n",
        "\n",
        "E. J. Candes. The restricted isometry property and its implications for compressed sensing. Comptes rendus mathematique, 346(9-10):589â€“592, 2008.\n",
        "\n",
        "E. J. Candes and T. Tao. Decoding by linear programming. IEEE transactions on\n",
        "information theory, 51(12):4203â€“4215, 2005.\n",
        "\n",
        "E. J. CandÃ¨s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on information theory, 52(2):489â€“509, 2006.\n",
        "\n",
        "S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20(1):33â€“61, 1998.\n",
        "\n",
        "D. L. Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4): 1289â€“1306, 2006.\n",
        "\n",
        "M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun, K. F. Kelly, and\n",
        "R. G. Baraniuk. Single-pixel imaging via compressive sampling. IEEE signal processing magazine, 25(2):83â€“91, 2008.\n",
        "\n",
        "S. Foucart and H. Rauhut. An invitation to compressive sensing. In A mathematical introduction to compressive sensing, pages 1â€“39. Springer, 2013.\n",
        "\n",
        "G. M. Gibson, S. D. Johnson, and M. J. Padgett. Single-pixel imaging 12 years on: a review. Optics Express, 28(19):28190â€“28208, 2020.\n",
        "\n",
        "T. Hastie, R. Tibshirani, and M. Wainwright. Statistical learning with sparsity: the lasso and generalizations. Chapman and Hall/CRC, 2019.\n",
        "\n",
        "Y. Plan and R. Vershynin. One-bit compressed sensing by linear programming. Communications on Pure and Applied Mathematics, 66(8):1275â€“1297, 2013.\n",
        "\n",
        "R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267â€“288, 1996.\n",
        "\n",
        "M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019."
      ]
    }
  ]
}