{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of lasso_and_restricted_eigenvalue.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emglUHu3XTEg"
      },
      "source": [
        "# Restricted Eigenvalue Condition for the Lasso\n",
        "\n",
        "<font color='green'> In this practical session, we introduce the restricted eigenvalue condition and its role in sparse linear prediction and estimation. Our main objectives are the following:\n",
        "</font>\n",
        "- <font color='green'>revisiting the basic inequality proof technique assuming the restricted eigenvalue condition;</font>\n",
        "- <font color='green'>implementing the proximal gradient method for the lasso objective and understanding how the restricted eigenvalue constant affects the convergence speed;</font>\n",
        "- <font color='green'>understanding why the lasso prediction performance can be sensitive to the restricted eigenvalue constant;</font>\n",
        "- <font color='green'>introducing an open problem regarding the existence of a computational-statistical gap for the problem of optimal in-sample sparse linear prediction.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh7NNRLRbaSj"
      },
      "source": [
        "We will consider the design matrix $X \\in \\mathbb{R}^{n \\times d}$ to be fixed and the observations generated via the following model:\n",
        "$$\n",
        "  y = Xw^{\\star} + \\xi\\text{, where }w^{\\star}\\in\\mathbb{R}^{d}\\text{ is a }k\\text{-sparse target vector and }\\xi \\sim N(0, \\sigma^{2}I_{n \\times n})\\text{ is the noise random variable.}\n",
        "$$\n",
        "Let $S \\subseteq \\{1, \\dots, d\\}$ denote the support set of non-zero coordinates of the signal vector $w^{\\star}$ (thus, $|S| = k$); we also denote $S^{c} = \\{1, \\dots, d\\} \\backslash S$.\n",
        "\n",
        "Define the cone\n",
        "$$\n",
        "  \\mathcal{C}(S) = \\{ \\Delta \\in \\mathbb{R}^{d} : \\|\\Delta_{S^{c}}\\|_{1} \\leq 3\\|\\Delta_{S}\\|_{1}\\}.\n",
        "$$\n",
        "Notice that the above cone is almost the same as the cone used in the definition of the restricted nullspace property (cf. the compressed sensing notebook). The only difference is that above we have a factor $3$ instead of $1$. Why this is the case will become clear shortly, when we obtain performance bounds for the lasso. Let us now define the restricted eigenvalue condition.\n",
        "\n",
        "---\n",
        "\n",
        "**Restricted Eigenvalue Condition**\n",
        "\n",
        "\n",
        "A matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfies the $\\gamma$-restricted eigenvalue condition with respect to the support set $S \\subseteq \\{1, \\dots, d\\}$ if \n",
        "$$\n",
        "  \\text{for any } \\Delta \\in \\mathcal{C}(S)\\text{ we have }\n",
        "  \\frac{1}{n}\\|X\\Delta\\|_{2}^{2} \\geq \\gamma \\|\\Delta\\|_{2}^{2}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "<font color='green'>**We remark that the restricted eigenvalue condition is significantly weaker than the restricted isometry assumption.**</font>\n",
        "Obtaining the latter requires sampling the rows of $X$ from isotropic distributions. On the other hand, the restricted eigenvalue condition can be satisfied when the rows are sampled from distributions will ill-conditioned covariance structures; refer to the bibliographic remarks section for further details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebkp92mPlILV"
      },
      "source": [
        "Denote the empirical risk functional by $R(w) = \\frac{1}{n}\\|Xw-y\\|_{2}^{2}$ and let the lasso objective be denoted by $R_{\\lambda}(w) = R(w) + \\lambda\\|w\\|_{1}$. Let $w_{\\lambda}$ denote any solution to the lasso objective:\n",
        "$$\n",
        "  w_{\\lambda} \\in \\text{argmin}_{w \\in \\mathbb{R}^{d}} R_{\\lambda}(w).\n",
        "$$\n",
        "Let us now see how restricted eigenvalue condition allows us to obtain *fast rate* convergence bounds on:\n",
        "1. the *estimation error* defined as $\\|w_{\\lambda} - w^{\\star}\\|_{2}^{2}$;\n",
        "2. the *in-sample prediction error* defined as $\\frac{1}{n} \\|Xw_{\\lambda} - Xw^{\\star}\\|_{2}^{2}$.\n",
        "\n",
        "<font color='green'>**As it turns out, obtaining upper bounds on both performance measures defined above can be done in just a few lines of analysis via the basic-inequality proof technique.**</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Obtaining Performance Bounds via Basic-Inequality Proof Technique**\n",
        "\n",
        "In what follows, we work under the two following conditions:\n",
        "- we suppose that our fixed design matrix $X$ satisfies the $\\gamma$-restricted eigenvalue condition with respect to the support set $S$ of the true parameter $w^{\\star}$;\n",
        "- we suppose that the parameter $\\lambda$ definining the lasso objective satisfies $\\lambda \\geq 4\\|\\frac{1}{n}X^{\\mathsf{T}}\\xi\\|_{\\infty}$.\n",
        "\n",
        "The idea of the basic-inequality proof technique is to combine the facts that $w_{\\lambda}$ is an *optimal* point for the objective $R_{\\lambda}$, while $w^{\\star}$ is a *feasible* point. Hence, we have\n",
        "$$\n",
        "  R_{\\lambda}(w_{\\lambda}) \\leq R_{\\lambda}(w^{\\star}).\n",
        "$$\n",
        "Denote $\\Delta = w^{\\star} - w_{\\lambda}$. Then, rearranging the above inequality yields\n",
        "\\begin{align*}\n",
        " \\underbrace{\\frac{1}{n}\\|X\\Delta\\|_{2}^{2}}_{\\text{in-sample prediction error}}\n",
        " &\\leq\n",
        " 2\\langle \\Delta, \\frac{1}{n}X^{\\mathsf{T}}\\xi\\rangle\n",
        " + \\lambda(\\|w^{\\star}\\|_{1} - \\|w_{\\lambda}\\|_{1}) \\\\\n",
        " &\\leq\n",
        " 2\\|\\Delta\\|_{1}\\|\\frac{1}{n}X^{\\mathsf{T}}\\xi\\|_{\\infty} \n",
        " + \\lambda(\\|w^{\\star}\\|_{1} - \\|w_{\\lambda}\\|_{1}) \\\\\n",
        " &=\n",
        " 2\\|\\Delta\\|_{1}\\|\\frac{1}{n}X^{\\mathsf{T}}\\xi\\|_{\\infty} \n",
        " + \\lambda(\\|\\Delta_{S} + (w_{\\lambda})_{S}\\|_{1} - \\|w_{\\lambda}\\|_{1}) \\\\\n",
        " &=\n",
        " 2\\|\\Delta\\|_{1}\\|\\frac{1}{n}X^{\\mathsf{T}}\\xi\\|_{\\infty} \n",
        " + \\lambda(\\|\\Delta_{S} + (w_{\\lambda})_{S}\\|_{1} - \\|(w_{\\lambda})_{S}\\|_{1} - \\|\\Delta_{S^{c}}\\|_{1}) \\\\\n",
        " &\\leq\n",
        " 2\\|\\Delta\\|_{1}\\|\\frac{1}{n}X^{\\mathsf{T}}\\xi\\|_{\\infty} \n",
        " + \\lambda(\\|\\Delta_{S}\\|_{1} - \\|\\Delta_{S^{c}}\\|_{1}) \\\\\n",
        " &\\leq\n",
        " \\frac{\\lambda}{2}\\|\\Delta\\|_{1}\n",
        " + \\lambda(\\|\\Delta_{S}\\|_{1} - \\|\\Delta_{S^{c}}\\|_{1}) \\\\\n",
        " &=\n",
        " \\frac{\\lambda}{2}(3\\|\\Delta_{S}\\|_{1} - \\|\\Delta_{S^{c}}\\|_{1}).\n",
        "\\end{align*}\n",
        "Since the left-hand side is non-negative, the above inequality implies that $\\Delta \\in \\mathcal{C}(S)$; thus, we may apply the $\\gamma$-restricted eigenvalue condition.\n",
        "\n",
        "We can thus obtain an upper bound on the estimation error as follows:\n",
        "\\begin{align*}\n",
        "  &\\gamma \\|\\Delta\\|_{2}^{2}\n",
        "  \\leq \\frac{1}{n}\\|X\\Delta\\|_{2}^{2}\n",
        "  \\leq \\frac{\\lambda}{2}\\cdot 3\\sqrt{k}\\|\\Delta\\|_{2} \\\\\n",
        "  \\implies&\n",
        "  \\|\\Delta\\|_{2}^{2} \\leq \\frac{1}{\\gamma^{2}}\\frac{9}{4} k \\lambda^{2}.\n",
        "\\end{align*}\n",
        "\n",
        "Similarly, an upper bound on the in-sample prediction error can be obtained by noting that\n",
        "\\begin{align*}\n",
        "  &\\frac{1}{n}\\|X\\Delta\\|_{2}^{2}\n",
        "  \\leq \n",
        "   \\frac{\\lambda}{2}\\cdot 3\\sqrt{k}\\frac{1}{\\sqrt{\\gamma}}\\cdot \\sqrt{\\gamma}\\|\\Delta\\|_{2}\n",
        "   \\leq\n",
        "   \\frac{\\lambda}{2}\\cdot 3\\sqrt{k}\\frac{1}{\\sqrt{\\gamma}}\\cdot \\frac{1}{\\sqrt{n}}\\|X\\Delta\\|_{2}\n",
        "  \\\\\n",
        "  \\implies&\n",
        "  \\frac{1}{n}\\|X\\Delta\\|_{2}^{2} \\leq  \\frac{1}{\\gamma}\\frac{9}{4}k \\lambda^{2}.\n",
        "\\end{align*}\n",
        "\n",
        "Finally, regarding the choice of the parameter $\\lambda$, recall that we have imposed a condition $\\lambda \\geq 4\\|\\frac{1}{n}X^{\\mathsf{T}}\\xi\\|_{\\infty}$.\n",
        "*Suppose that the $\\ell_{2}$ norms of the columns of $\\frac{1}{\\sqrt{n}}X$ are at most equal to $1$*. The right-hand side of the previous equation can then be shown to be at most $8\\sigma\\sqrt{\\log d}/\\sqrt{n}$ with overwhelming probability and thus we will perform our simulations with the choice:\n",
        "$$\n",
        "  \\lambda = \\frac{8\\sigma\\sqrt{\\log d}}{\\sqrt{n}}.\n",
        "$$\n",
        "<font color='green'>**Note that the above choice of $\\lambda$ gives the \"fast rates\" for our above upper bounds on the estimation and prediction errors.**</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWuoJCKJXdn5"
      },
      "source": [
        "The rest of this notebook is organized as follows:\n",
        "- in the next section, we import some of the code developed in the \"optimization\" notebook;\n",
        "- we then implement a data generating distribution (sampling from the spiked identity model) that violates the restricted isometry property but satisfies the restricted eigenvalue property;\n",
        "- in the \"Computational Considerations\" section, we introduce an exercise that asks us to explore the convergence properties of proximal gradient descent when applied to a lasso objective satisfying restricted eigenvalue condition.\n",
        "- in the \"Statistical Considerations\" section, we introduce an exercise that asks us to find a design matrix for which the lasso incurs a sub-optimal convergence rate for the in-sample prediction error rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcofGQEGSz74"
      },
      "source": [
        "## Reusing Code From the \"Optimization\" Practical Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSMFXaALezsz"
      },
      "source": [
        "We import the abstract `Optimizer` class and the implementation of `GradientDescent` subclass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_3MDVzZiMY"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "tnp.experimental_enable_numpy_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIwW-uDaSx8R"
      },
      "source": [
        "class Optimizer(object):\n",
        "  \"\"\" A base class for optimizers. \"\"\"\n",
        "\n",
        "  def __init__(self, eta):\n",
        "    \"\"\" :eta_t: A function taking as argument the current iteration t and\n",
        "          returning the step size eta_t to be used in the current iteration. \"\"\"\n",
        "    super().__init__()\n",
        "    self.eta = eta\n",
        "    self.t = 0 # Set iterations counter.\n",
        "\n",
        "  def apply_gradient(self, x_t, g_t):\n",
        "    \"\"\" Given the current iterate x_t and gradient g_t, updates the value\n",
        "      of x_t to x_(t+1) by performing one iterative update.\n",
        "        :x_t: A tf.Variable which value is to be updated.\n",
        "        :g_t: The gradient value, to be used for performing the update.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"To be implemented by subclasses.\")\n",
        "\n",
        "  def step(self, f, x_t):\n",
        "    \"\"\" Updates the variable x_t by performing one optimization iteration.\n",
        "        :f: A function which is being minimized.\n",
        "        :x_t: A tf.Variable with respect to which the function is being\n",
        "          minimized and which value is to be updated\n",
        ".\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      fx = f(x_t)\n",
        "    g_t = tape.gradient(fx, x_t)\n",
        "    self.apply_gradient(x_t, g_t)\n",
        "    # Update the iterations counter.\n",
        "    self.t += 1\n",
        "\n",
        "  def optimize(self, f, x_t, n_iterations):\n",
        "    \"\"\" Applies the function step n_iterations number of times, starting from\n",
        "      the iterate x_t. Note: the number of iterations member self.t is not\n",
        "      restarted to 0, which may affects the computed step sizes. \n",
        "        :f: Function to optimize.\n",
        "        :x_t: Current iterate x_t.\n",
        "        :returns: A list of length n_iterations+1, containing the iterates\n",
        "          [x_t, x_{t+1}, ..., x_{t+n_iterations}].\n",
        "    \"\"\"\n",
        "    x = tf.Variable(x_t)\n",
        "    iterates = []\n",
        "    iterates.append(x.numpy().reshape(-1,1))\n",
        "    for _ in range(n_iterations):\n",
        "      self.step(f, x)\n",
        "      iterates.append(x.numpy().reshape(-1,1))\n",
        "    return iterates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOnOpK4AUBAt"
      },
      "source": [
        "class GradientDescent(Optimizer):\n",
        "  \"\"\" An implementation of gradient descent uptades. \"\"\"\n",
        "\n",
        "  def apply_gradient(self, x_t, g_t):\n",
        "    eta_t = self.eta(self.t)\n",
        "    x_t.assign_add(-eta_t * g_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-WFT2FkGG11"
      },
      "source": [
        "## Sampling Data From the Spiked Identity Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEBL7HzP-Y8r"
      },
      "source": [
        "In this section, we implement a class `Problem` that implements various methods concerning the fixed-design linear regression setup, such as computing the empirical risk, computing the lasso parameter value $\\lambda$ suggested in the introduction, and resampling of the labels. Notice that the design matrix $X$ is assumed to be fixed, and the only source of randomness comes from the noise random variables $\\xi$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hZawsRTUDi0"
      },
      "source": [
        "class Problem(object):\n",
        "  \"\"\" A class for representing a dataset (X, y) and providing methods for\n",
        "  computing the empirical risk function and the in-sample prediction error. \"\"\"\n",
        "  \n",
        "  def __init__(self, X, w_star, noise_std):\n",
        "    \"\"\" :n: An n \\times d matrix of covariates.\n",
        "        :w_star: A d-dimensional vector used to generate noisy observations\n",
        "          y_i = <w_star, x_i> + N(0, noise_std**2). In this practical session\n",
        "          w_star should be a sparse vector.\n",
        "        :noise_std: Standard deviation of the zero-mean Gaussian label noise.\n",
        "    \"\"\"\n",
        "    self.X = X\n",
        "    self.n = X.shape[0]\n",
        "    self.d = X.shape[1]\n",
        "    self.w_star = w_star.reshape(self.d, 1)\n",
        "    self.noise_std = noise_std\n",
        "    self.generate_labels(seed=0)\n",
        "\n",
        "  def generate_labels(self, seed):\n",
        "    \"\"\" Resamples new labels y = Xw* + noise_std*N(0, I) and stores it as a\n",
        "      class member self.y. \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    self.xi = np.random.normal(loc=0.0, scale=1.0, size=(self.n, 1))\n",
        "    self.y = self.X @ self.w_star + self.xi\n",
        "\n",
        "  def get_lasso_lambda(self):\n",
        "    \"\"\" Returns the lasso lambda computed in the introduction. \"\"\"\n",
        "    return 8.0*self.noise_std*np.sqrt(np.log(self.d))/np.sqrt(self.n)\n",
        "\n",
        "  def compute_in_sample_prediction_error(self, w):\n",
        "    \"\"\" For a d-dimensional vector w outputs 1/n ||X(w - w*)||_{2}^{2}. \"\"\"\n",
        "    return tnp.average((self.X @ (w - self.w_star))**2)\n",
        "\n",
        "  def compute_empirical_risk(self, w):\n",
        "    \"\"\" For a d-dimensional vector w, outputs R(w) = 1/n ||Xw - y||_{2}^{2}. \"\"\"\n",
        "    return tnp.average((self.X @ w - self.y)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmsXPgWD_cbR"
      },
      "source": [
        "Next, we implement a function for obtaining a design matrix $X$, whose rows are randomly sampled from the spiked identity model:\n",
        "$$\n",
        "  X_{i} \\sim N(0, \\gamma I_{d} + (1-\\gamma)\\mathbf{1}\\mathbf{1}^{\\mathsf{T}}).\n",
        "$$\n",
        "For $\\gamma > 0$ bounded away from $1$, random matrices with rows sampled from the above distribution do not satisfy the restricted isometry property but satisfy the restricted eigenvalue condition. See Example 7.18 in the textbook by *Wainwright [2019]* for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBl5k0C5BM2o"
      },
      "source": [
        "def get_spiked_identity_X(n, d, gamma):\n",
        "  \"\"\" See the discussion in the above text cell. \"\"\"\n",
        "  X_id = np.random.normal(0, 1, size=(n,d)) # Identity covariance component.\n",
        "  X_spiked = np.ones((n, d)) * np.random.normal(0,1,size=(n,1)) # Spiked component.\n",
        "  return gamma*X_id + (1.0-gamma)*X_spiked"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljYyWeP_b5h6"
      },
      "source": [
        "## Computational Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mrP2HWMb9Bv"
      },
      "source": [
        "Recall that the lasso objective can be written as a sum of a smooth and non-smooth terms\n",
        "$$\n",
        "  R_{\\lambda}(w) = \\underbrace{\\frac{1}{n}\\|Xw - y\\|_{2}^{2}}_{\\text{smooth}} + \n",
        "  \\underbrace{\\lambda\\|w\\|_{1}}_{\\text{non-smooth}}.\n",
        "$$\n",
        "Due to the $\\ell_{1}$ penalty, the overall objective $R_{\\lambda}$ is non-smooth. Based on the results introduced in the optimization lectures, it would appear that only the slow convergence rate of order $O(1/\\sqrt{t})$ is possible. To circumvent this issue, in Lecture 10 we have introduced <font color='green'>**proximal algorithms that allow us to obtain the smooth convergence rate of order $O(1/t)$ for non-smooth problems whenever we can explicitly compute the proximal operator**</font>. In the following exercise, we compare the usual gradient descent updates with the smooth gradient descent updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RhVJ5_KB9CG"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "n=250\n",
        "d=1000\n",
        "w_star = np.zeros((d,1))\n",
        "k = 10\n",
        "w_star[:k,0] = 10.0\n",
        "X = get_spiked_identity_X(n, d, gamma=1.0)\n",
        "\n",
        "problem = Problem(\n",
        "    X=X,\n",
        "    w_star=w_star,\n",
        "    noise_std=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWyqjXGRsvXc"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbPus8WwsxC5"
      },
      "source": [
        "- In you simulations, use the python variable `problem` defined in the preceding cell.\n",
        "- Use the lasso penalty lamba given by `problem.get_lasso_lambda()`.\n",
        "- Implement the ISTA algorithm described in Lecture 10.\n",
        "- Run ISTA and gradient descent with a constant step size $\\eta = 0.1$ for $T=500$ iterations each. Plot the value of $\\|w_{t} - w^{\\star}\\|_{2}$ computed by both algorithms against the number of iterations $t$. Commend on your observations.\n",
        "- Repeat the above simulations with different choices of $\\gamma$ parameter. What happens for values of $\\gamma$ close to $0$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm1IDPeLhGuS"
      },
      "source": [
        "## Statistical Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGx_8rLmWoIT"
      },
      "source": [
        "Recall the upper bound on the in-sample prediction error proved at the beginning of this notebook. With the regularization parameter $\\lambda = \\frac{8\\sigma\\sqrt{\\log d}}{\\sqrt{n}}$, we obtained\n",
        "$$\n",
        "  \\underbrace{\\frac{1}{n}\\|Xw_{\\lambda} - Xw^{\\star}\\|_{2}^{2}}_{\\text{in-sample prediction error}}\n",
        "  \\leq \\frac{144}{\\gamma}\\frac{k\\sigma^{2}\\log d}{n}.\n",
        "$$\n",
        "While the above bound gives the fast rate, the issue is in the presence of the parameter $1/\\gamma$. While the dependence on the conditioning of the design matrix $X$ is perfectly natural for the estimation problem, it is less natural in the prediction setting. As an example, consider duplicating some columns in the design matrix of $X$ corresponding to indexes on the support set of the sparse signal vector $w^{\\star}$. Intuitively, duplicating columns makes the estimation problem impossible, as the model becomes unidentifiable, yet the prediction problem becomes easier, as there is no difference which of the duplicate columns the learning algorithm selects (i.e., the learning algorithm gets more freedom if some columns get duplicated).\n",
        "\n",
        "<font color='green'>**At the same time, it is known that the computationally intractable $\\ell_{0}$ estimator satisfies the fast rate in-sample prediction error bound $O(\\frac{k\\sigma^{2}\\log d}{n})$ without any dependence on the restricted eigenvalue constant $\\gamma$.**</font> This leaves two open possibilities: either the lasso estimator is sub-optimal or the analysis presented above is too loose.\n",
        "\n",
        "<font color='green'>**In the following exercise, you are asked to (informally) show that the lasso estimator is indeed sub-optimal by constructing a design matrix $X \\in \\mathbb{R}^{n \\times d}$ such that the lasso incurs in-sample prediction error of order $\\Omega(1/\\sqrt{n})$ with high probability.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMumAeQ6yelD"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs5zSa4HywA9"
      },
      "source": [
        "Let the signal vector $w^{\\star} \\in \\mathbb{R}^{d}$ be $2$-sparse with the first two coordinates equal to $1/2$. Construct a design matrix $X \\in \\mathbb{R}^{n \\times d}$ such that the lasso estimator $w_{\\lambda}$ with the usual choice of parameter\n",
        "$\\lambda = \\Theta(\\frac{\\sigma\\sqrt{\\log d}}{\\sqrt{n}})$ satisfies the following lower bound with high probability:\n",
        "$$\n",
        "  \\frac{1}{n}\\|X w_{\\lambda} - Xw^{\\star}\\|_{2}^{2} = \\Omega\\left(1/\\sqrt{n}\\right).\n",
        "$$\n",
        "You are only asked to establish the above result informally. Verify your construction empirically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwLur0LggPA7"
      },
      "source": [
        "### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57UijKVigZhF"
      },
      "source": [
        "To gain some intuition as to what a bad matrix would look like try answering the following questions:\n",
        "- What is the smallest value $\\lambda = \\lambda_{\\text{min}}$ that guarantees $(w_{\\lambda})_{S^{c}} = 0$ (i.e., the lasso output does not fit any coordinates outside of the true support $S$)?\n",
        "- What happens for values of $\\lambda$ significantly smaller than $\\lambda_{\\text{min}}$?\n",
        "- Consider the case $n=d$ and let $X$ be a block-diagonal matrix constructed from $2 \\times 2$ matrices $A \\in \\mathbb{R}^{2 \\times 2}$ (i.e., each block is the same matrix repeated). Choose the matrix $A$ such that $(w_{\\lambda})_{1} \\neq 0$ and $(w_{\\lambda})_{2} \\neq 2$ if an only if $\\lambda \\ll \\lambda_{\\text{min}}$.\n",
        "- Deduce the desired slow rate lower bound."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnxGsJRf1_OS"
      },
      "source": [
        "## Bibliographic Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6zkTcbi2Bi3"
      },
      "source": [
        "A slightly weaker version of the restricted eigenvalue condition than the one presented in this practical session is due to *Bickel, Ritov, and Tsybakov [2009]*. Our presentation of the lasso estimation error and in-sample prediction error upper bounds is based on [*Wainwright, 2019, Chapter 7*], which in turn credits *Bickel, Ritov, and Tsybakov [2009]*. Many variations of different conditions used to establish statistical performance bounds for the lasso exist; the\n",
        "interplay between these different conditions is explored by *van de Geer and Bühlmann [2009]*. The fact that non-isotropic random ensembles can satisfy the restricted eigenvalue condition is shown in [*Raskutti, Wainwright, and Yu, 2010, Rudelson and Zhou, 2012, Oliveira, 2016*]. *Agarwal, Negahban, and Wainwright [2012]* establish the geometric convergence of gradient methods under restricted notions of strong convexity and smoothness, as seen in Exercise 1 of this practical session. Slow rate lower bounds for the lasso in-sample prediction error were shown by *Foygel and Srebro [2011]* and *Dalalyan, Hebiri, and Lederer [2017]*. The block-diagonal construction of the ill-behaved design matrix presented in this notebook is due to *Candès and Plan [2009]*. This construction was later refined by *Zhang, Wainwright, and Jordan [2017]* to show that the sub-optimal slow rate is intrinsic for regularization paths of a large class of penalized estimators, including the lasso. The same authors have also previously shown that any polynomial-time algorithm constrained to output a sparse vector cannot achieve the fast rate without imposing strong restrictions on the design [*Zhang, Wainwright, and Jordan, 2014*]. In contrast, we remind that the computationally intractable $\\ell_{0}$ estimator achieves the fast rate without any restrictions on the design matrix other than column normalization. Whether there exists a polynomial-time algorithm achieving the fast rate guarantee under the same conditions as the $\\ell_{0}$ estimator remains an open problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLx7Knxw72W6"
      },
      "source": [
        "**References**\n",
        "\n",
        "A. Agarwal, S. Negahban, and M. J. Wainwright. Fast global convergence of gradient methods for high-dimensional statistical recovery. The Annals of Statistics, pages 2452–2482, 2012.\n",
        "\n",
        "P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and dantzig selector. The Annals of statistics, 37(4):1705–1732, 2009.\n",
        "\n",
        "E. J. Candès and Y. Plan. Near-ideal model selection by l1 minimization. The Annals of Statistics, 37(5A):2145–2177, 2009.\n",
        "\n",
        "A. S. Dalalyan, M. Hebiri, and J. Lederer. On the prediction performance of the lasso. Bernoulli, 23(1):552–581, 2017.\n",
        "\n",
        "R. Foygel and N. Srebro. Fast-rate and optimistic-rate error bounds for l1-regularized regression. arXiv preprint arXiv:1108.0373, 2011.\n",
        "\n",
        "R. Oliveira. The lower tail of random quadratic forms with applications to ordinary least squares. Probability Theory and Related Fields, 166(3-4):1175–1194, 2016.\n",
        "\n",
        "G. Raskutti, M. J. Wainwright, and B. Yu. Restricted eigenvalue properties for correlated gaussian designs. The Journal of Machine Learning Research, 11:2241–2259, 2010.\n",
        "\n",
        "M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements. In\n",
        "Conference on Learning Theory, pages 10–1. JMLR Workshop and Conference Proceedings, 2012.\n",
        "\n",
        "S. A. van de Geer and P. Bühlmann. On the conditions used to prove oracle results for the lasso. Electronic Journal of Statistics, 3:1360–1392, 2009.\n",
        "\n",
        "M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.\n",
        "\n",
        "Y. Zhang, M. J. Wainwright, and M. I. Jordan. Lower bounds on the performance of\n",
        "polynomial-time algorithms for sparse linear regression. In Conference on Learning Theory, pages 921–948, 2014.\n",
        "\n",
        "Y. Zhang, M. J. Wainwright, and M. I. Jordan. Optimal prediction for sparse linear models? lower bounds for coordinate-separable m-estimators. Electronic Journal of Statistics, 11 (1):752–799, 2017."
      ]
    }
  ]
}